<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SIR-Smart Iris Recognition</title>
    <link>https://hycasia.github.io/</link>
      <atom:link href="https://hycasia.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>SIR-Smart Iris Recognition</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hycasia.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_2.png</url>
      <title>SIR-Smart Iris Recognition</title>
      <link>https://hycasia.github.io/</link>
    </image>
    
    <item>
      <title>Example Page 1</title>
      <link>https://hycasia.github.io/courses/example/example1/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://hycasia.github.io/courses/example/example1/</guid>
      <description>&lt;p&gt;In this tutorial, I&amp;rsquo;ll share my top 10 tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-1&#34;&gt;Tip 1&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-2&#34;&gt;Tip 2&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Page 2</title>
      <link>https://hycasia.github.io/courses/example/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://hycasia.github.io/courses/example/example2/</guid>
      <description>&lt;p&gt;Here are some more tips for getting started with Academic:&lt;/p&gt;
&lt;h2 id=&#34;tip-3&#34;&gt;Tip 3&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
&lt;h2 id=&#34;tip-4&#34;&gt;Tip 4&lt;/h2&gt;
&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://hycasia.github.io/talk/example/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/talk/example/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Academic&amp;rsquo;s 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further talk details can easily be added to this page using &lt;em&gt;Markdown&lt;/em&gt; and $\rm \LaTeX$ math code.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Iris Recognition</title>
      <link>https://hycasia.github.io/project/iris-recognition/</link>
      <pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/project/iris-recognition/</guid>
      <description>&lt;div style=&#34;WIDTH:100%; HEIGHT:100%&#34;&gt;
&lt;font color=#00BFFF size = 5 &gt;&lt;strong&gt; &amp;emsp;&amp;emsp;虹膜识别是进一步提升安全性、可靠性的必由之路&lt;/strong&gt; &amp;emsp;&amp;emsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;/font&gt;
&lt;font color=#00BFFF size = 5 &gt;&lt;strong&gt; &amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;———谭铁牛院士&lt;/strong&gt;&lt;/font&gt;  
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;虹膜识别是最具潜力的生物识别方法之一，是识别率高、非接触性、防欺骗性好的识别方法。虹膜属于人眼的一部分，如下图所示。&lt;/font&gt;
&lt;div style=&#34;float:right; clear: both;&#34; align=&#34;center&#34;&gt;&lt;img src=&#34;iris.png&#34; width=&#34;300&#34; alt=&#34;&#34; hspace=&#34;8&#34;&gt;&lt;br /&gt;&lt;/div&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;人眼的外观主要由巩膜、虹膜、瞳孔三部分组成，其中巩膜即眼球外围的白色部分，约占人眼总面积的30%，眼睛的中心为瞳孔部分，约占5%，虹膜位于巩膜和瞳孔之间，约占整个眼睛的65%，包含了最丰富的纹理信息，外观上看，虹膜由许多腺窝、褶皱、色素斑等构成，是人体中最独特的结构之一。因为瞳孔、虹膜和巩膜一般颜色不同，灰度值呈梯度变化，所以根据它们灰度不同，可以将它们明显分开。从几何形状可以看出，虹膜的内、外边界可以近似为圆形，这使它具有易检测性。临床观察发现：虹膜在人的一生当中几乎不发生变化，只有很少的虹膜纹理可能会由于年龄或者外伤导致纹理破坏。&lt;/font&gt;&lt;/div&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;作为表示个人身份的标识物，必须具备作为身份标识的重要特征。人脸、指纹等许多生物特征具有作为身份标识的特性，但是，虹膜在这些特性方面表现的更为突出，具有许多先天优势，是其他生物特征无法与之媲美的。&lt;/font&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&lt;ul type=&#39;circle&#39;&gt;&lt;li&gt;普遍性————虹膜是每个人天生都具有的。&lt;/li&gt;&lt;li&gt;唯一性————虹膜的纤维组织细节复杂而丰富，每个人错综复杂的虹膜独一无二，只与虹膜的形成过程有关。&lt;/li&gt;&lt;li&gt;稳定性————虹膜从婴儿胚胎发育的第三个月起开始发育，到第八个月虹膜的主要纹理结构已经成形。&lt;/li&gt;&lt;li&gt;非入侵检测————从一定距离即可获得虹膜数字图像，无需用户接触设备。&lt;/li&gt;&lt;li&gt;可接受程度好————虹膜识别以其认证准确度高、速度快、安全性高，被用户所接受。&lt;/li&gt;&lt;li&gt;可检测性————利用图像处理技术检测出虹膜边界，易于拟合分割和和归一化&lt;/li&gt;&lt;li&gt;防伪性高————虹膜的半径小，在可见光下中国人的虹膜图像呈现深褐色，看不到纹理信息，需要虹膜图像专业采集设备和用户的配合，所以一般情况下很难被盗取&lt;/li&gt;&lt;li&gt;防欺骗性好————虹膜的唯一性决定了不同人眼的虹膜很难被冒充模仿。&lt;/li&gt;&lt;/ul&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&amp;emsp;&amp;emsp;生物特征识别通过捕获生物样本，然后采用数学方法把样本转化成相同大小的模板，提取有效的可区别性特征，就可以客观地和其他一个完整的虹膜身份识别系统主要由四个部分组成：虹膜图像获取、虹膜图像预处理、虹膜特征提取、模式比对。
&lt;/font&gt;
&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 5 &gt;&lt;ol&gt;&lt;li&gt;虹膜图像获取&lt;/li&gt;  
&amp;emsp;虹膜图像采集的目的是为了获取有效的虹膜图像，在传统的虹膜识别场景中，通常采用专业的成像装置在近红外光（波长700nm到900nm）照射和用户的配合下才能捕获清晰的高分辨率虹膜图像。近些年来，随着光学镜头、传感器和计算成像技术等的发展，虹膜识别的可用距离不断变大，相关装置也变得越来越轻巧实用，“远距离”、“行进中”、“移动端”和“可见光下”等少约束场景的虹膜识别对于用户使用时的约束越来越少，极大地提升了虹膜识别应用范围和用户友好性。
&lt;li&gt;虹膜图像预处理&lt;/li&gt;
虹膜图像除了必须的虹膜区域以外，也包含了诸如巩膜、睫毛、瞳孔等非虹膜区域，因此不能直接用于虹膜识别。其次，一些噪声茹照明变化、睫毛遮挡、镜面反射、瞳孔放缩等会明显增加虹膜的类内差异，降低虹膜的识别率。常规的虹膜预处理步骤包括：虹膜活体检测、虹膜图像质量评估、虹膜分割、虹膜归一化和虹膜图像增强。
&lt;li&gt;虹膜图像特征分析&lt;/li&gt;
虹膜图像特征分析主要包含两部分：特征提取和对比分类。虹膜特征提取是指从归一化的虹膜图像中提取紧凑有区分的虹膜特征，然后使用计算机可以存储和读取的格式进行编码。虹膜比对和分类(或者称为匹配)是指将提取的虹膜特征编码和事先在数据库注册过的虹膜特征编码通过某种相似性度量比如汉明距离、余弦距离进行对比，计算相似性分数，依次确定用户身份。
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-Iris-Complex</title>
      <link>https://hycasia.github.io/dataset/casia-iris-complex/</link>
      <pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/dataset/casia-iris-complex/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Iris is considered one of the most accurate and reliable biometric modality. Iris is more stable and distinctive compared with fingerprint, face, voice, etc, and difficult to be replicated for spoof attacks. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. In practical applications, the iris recognition system must face various unpredictable iris image degraded. For example, recognition of low-quality iris images, non-cooperative iris images, long-range iris images, and moving iris images are all huge problems in iris recognition. We believe that the first step in solving these problems is to design and develop a database of iris images that includes all of these degraded.&lt;/p&gt;
&lt;h2 id=&#34;brief-descriptions-and-statistics-of-the-database&#34;&gt;Brief Descriptions and Statistics of the Database&lt;/h2&gt;
&lt;p&gt;CASIA-Iris-Complex contains totally 22,932 images from 292 Asian subjects. It includes two subsets: CASIA-Iris-CX1 and CASIA-Iris-CX2. All images were collected under NIR illumination and two eyes were captured simultaneously. Detailed information of each subset is shown below.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Subset Characteristics&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;CASIA-Iris-CX1&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;CASIA-Iris-CX2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Sensor&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Canon EOS 1300D&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;MindVision    MV-SUF1200GM-T&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Environment&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Indoor with   brightness change&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Indoor&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;No. of subjects&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;255&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;37&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;No. of classes&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;510&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;73&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;No. of images&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;18785&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4147&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Resolution&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;4608*3456&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;640*480&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Format&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;jpeg&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;bmp&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Distance&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.75m&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;1m,3m,5m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;Object&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Partial face&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Eye&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;casia-iris-cx1&#34;&gt;CASIA-Iris-CX1&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-CX1 is designed to explore quality degrade caused by subject such as pupil dilation, strabismus, and occlusion. Partial face images were captured with a modified Canon EOS 1300D camera which Infrared cut-off filters was replaced by Infrared passing filters. All collection work was finished indoors, and both visible light source and near-infrared light source are used for illumination.
&lt;img src=&#34;./cx1camera.png&#34; alt=&#34;environment and equipment&#34;&gt;&lt;/p&gt;
&lt;p&gt;For pupil dilation, we changed the intensity of visible light to stimulate pupil variety while keeping the intensity of infrared light unchanged. In addition, we also collected images under natural light, such as noon, dusk, sunny and cloudy.
&lt;img src=&#34;./cx1brightness.png&#34; alt=&#34;pupil dilation&#34;&gt;&lt;/p&gt;
&lt;p&gt;For strabismus, we placed five targets behind the camera, respectively at the left, top left, top, top right, and right positions. Looking at these targets will cause a squint of about 45 degrees.
&lt;img src=&#34;./cx1offangle.jpg&#34; alt=&#34;off-angle&#34;&gt;&lt;/p&gt;
&lt;p&gt;For occlusion, Volunteers were required to wear glasses, mask and using hand to cover mouth. During this section, the near-infrared light source is randomly moved to generate light spot. Meanwhile some glasses also have stains on the surface to occlude iris. Another type of occlusion comes from the person being collected. Volunteers were required to squint or close their eyes. We noticed a more interesting problem is that for some elderly volunteers, their eyelids will naturally sag, resulting in very serious occlusion.
&lt;img src=&#34;./cx1occlusion.jpg&#34; alt=&#34;occlusion&#34;&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, both defocus and motion blur are inevitable during the acquisition process, and we retain these images for research on related issues.&lt;/p&gt;
&lt;h3 id=&#34;casia-iris-cx2&#34;&gt;CASIA-Iris-CX2&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-CX2 is a &lt;strong&gt;small-scale experimental&lt;/strong&gt; dataset used to explore the problems in long-distance iris recognition. We use a self-developed iris imaging system, which can obtain iris images with sufficient resolution (diameter greater than 60 pixels) in the range of 1-6m.
&lt;img src=&#34;./cx2camera.png&#34; alt=&#34;environment and equipment&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the case of a long distance, the image may be defocus blurred due to the error of distance perception. Therefore, we collected a continuous zoom image sequence which is changed as &amp;ldquo;blur-clear-blur“.&lt;/p&gt;
&lt;p&gt;Another problem is near-infrared lighting. The illuminance of a light source with a fixed power and a fixed position is different at different distances. In order to ensure the image consistency of the entire subdataset, we moved the near-infrared light source during the acquisition process and ensured that the brightness of the image was basically equal. At the same time, we also obtained a series of over-exposed/under-exposed images by varying the exposure time (8ms ~ 35ms).
&lt;img src=&#34;./cx2imgs.png&#34; alt=&#34;eyes&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;database-organization&#34;&gt;Database Organization&lt;/h2&gt;
&lt;p&gt;The file name of each image in CASIA-Iris-Complex is unique to each other and denotes some useful properties associated with the image such as subject ID, left/right eye, image ID etc.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-CX1 are stored as: AAAA_B_Y_Y_CD_DDD.jpg&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AAAA: the unique identifier of the subject in the subset.&lt;/li&gt;
&lt;li&gt;B: &amp;lsquo;1&amp;rsquo; denotes left eye and &amp;lsquo;2&amp;rsquo; denotes right eye.&lt;/li&gt;
&lt;li&gt;CC: represent the acquisition conditions.
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;2x&amp;rsquo; represents brightness, where x is between 0-4, means &amp;lsquo;natural light&amp;rsquo;, &amp;lsquo;dark&amp;rsquo;, &amp;lsquo;weak&amp;rsquo;, &amp;lsquo;medium&amp;rsquo;, and &amp;lsquo;strong&amp;rsquo; brightness.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;3x&amp;rsquo; represents the direction of thr person looks at, where x is between 1-4,  means left-up, right-up, right-down, left-down.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;40&amp;rsquo; and &amp;lsquo;41&amp;rsquo; represents squinting and closing eyes.&lt;/li&gt;
&lt;li&gt;&amp;lsquo;5x&amp;rsquo; represents occlusion, where x is the occlusion type, and the value is between 1-3, which in turn means that the hand covers the face, wears a mask, and wears glasses&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Note: Unless otherwise specified, the brightness of the image is &amp;ldquo;medium&amp;rdquo;, the subject is required to look directly at the camera and not wear obstructions such as glasses.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;DDD: the index of the image in the same scenarios&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-CX1 are stored as: AAAA_B_CD_TTTTTTTTTT.jpg&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AAAA: the unique identifier of the subject in the subset.&lt;/li&gt;
&lt;li&gt;B: &amp;lsquo;1&amp;rsquo; denotes left eye and &amp;lsquo;2&amp;rsquo; denotes right eye.&lt;/li&gt;
&lt;li&gt;C: the acquisition distance, and the value is between 1-5.&lt;/li&gt;
&lt;li&gt;D: the acquisition conditions.
&lt;ul&gt;
&lt;li&gt;&amp;lsquo;1&amp;rsquo; denotes normal;&lt;/li&gt;
&lt;li&gt;&amp;lsquo;2&amp;rsquo; denotes lens zoom;&lt;/li&gt;
&lt;li&gt;&amp;lsquo;3&amp;rsquo; denotes exposure time changes&lt;/li&gt;
&lt;li&gt;&amp;lsquo;4&amp;rsquo; denotes turning eyes;&lt;/li&gt;
&lt;li&gt;&amp;lsquo;5&amp;rsquo; denotes turning head;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;TTTTTTTTTT: the timestamp of this image.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;copyright-note-and-contacts&#34;&gt;Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as &amp;ldquo;Portions of the research in this paper use the CASIA-Iris-Complex-V1.0 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;To download this dataset, plase contact 
&lt;a href=&#34;mailto://sir@cripac.ia.ac.cn&#34;&gt;sir@cripac.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alignment Free and Distortion Robust Iris Recognition</title>
      <link>https://hycasia.github.io/publication/afdr-iris-recognition/afdr-iris-recognition/</link>
      <pubDate>Thu, 20 Aug 2020 07:42:58 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/afdr-iris-recognition/afdr-iris-recognition/</guid>
      <description>&lt;!-- 
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Cite&lt;/em&gt; button above to demo the feature to enable visitors to import publication metadata into their reference management software.
  &lt;/div&gt;
&lt;/div&gt;


&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click the &lt;em&gt;Slides&lt;/em&gt; button above to demo Academic&amp;rsquo;s Markdown slides feature.
  &lt;/div&gt;
&lt;/div&gt;


Supplementary notes can be added here, including [code and math](https://sourcethemes.com/# academic/docs/writing-markdown-latex/). --&gt;
</description>
    </item>
    
    <item>
      <title>MediaPipe Iris</title>
      <link>https://hycasia.github.io/post/mediapipe-iris/</link>
      <pubDate>Sat, 08 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/post/mediapipe-iris/</guid>
      <description>&lt;!--&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;10&#39;&gt;MediaPipe Iris 实时虹膜跟踪和深度估计&lt;/font&gt;&lt;/div&gt;    
&lt;div style=&#39;display:none&#39;&gt;
标题居中
&lt;/div&gt;--&gt;
 &lt;font size=&#39;5&#39;&gt;
 &amp;emsp;&amp;emsp;包括计算摄影（例如，人像模式和闪光反射）和增强现实效果（例如，虚拟化身）在内的大量实际应用程序都依赖于通过跟踪虹膜来估计眼睛位置。一旦获得了准确的虹膜跟踪，我们就可以确定从相机到用户的距离，而无需使用专用的深度传感器。反过来，这可以改善各种用例，从计算摄影到适当大小的眼镜和帽子的虚拟试戴，到根据视听者的距离采用字体大小的可用性增强。
 由于有限的计算资源，可变的光照条件和遮挡物（例如头发或人斜视）的存在，虹膜跟踪是在移动设备上解决的一项艰巨任务。通常，会使用复杂的专用硬件，从而限制了可在其中应用该解决方案的设备范围。
 &lt;/font&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://pic1.zhimg.com/v2-abc83ffe57801610f81c22845ec1a937_b.gif&#34; alt=&#34;show&#34; width=&#34;650&#34; height=&#34;230&#34; /&gt;
&lt;/div&gt;
&lt;div align=&#39;center&#39;&gt;由MediaPipe Iris实现的眼睛重新着色示例&lt;/div&gt; 
&lt;/div&gt;
 &lt;font size=&#39;5&#39;&gt;
&amp;emsp;&amp;emsp;谷歌日前发布了用于精确虹膜估计的全新机器学习模型：MediaPipe Iris。所述模型以MediaPipe Face Mesh的研究作为基础，而它无需专用硬件就能够通过单个RGB摄像头实时追踪涉及虹膜，瞳孔和眼睛轮廓的界标。利用虹膜界标，模型同时能够在不使用深度传感器的情况下以相对误差小于10％的精度确定对象和摄像头之间的度量距离。请注意，虹膜追踪不会推断人们正在注视的位置，同时不能提供任何形式的身份识别。MediaPipe是一个旨在帮助研究人员和开发者构建世界级机器学习解决方案与应用程序的开源跨平台框架，所以在MediaPipe中实现的这一系统能够支持大多数现代智能手机，PC，笔记本电脑，甚至是Web。
&lt;div&gt;
&lt;div align=center&gt;
&lt;img src=&#34;http://p3.itc.cn/q_70/images03/20200808/b4ad6d1e27c5402c9f6752ce39d9a24a.gif&#34; alt=&#34;show&#34; width=&#34;240&#34; height=&#34;270&#34;&gt;
&lt;/div&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定&lt;/div&gt; 
&lt;!-- 用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定--&gt; 
&lt;p&gt;&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;1. 用于虹膜追踪的机器学习管道&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;font size=&#39;5&#39;&gt;  谷歌介绍道，开发系统的第一步利用了之前针对3D Face Meshes的研究，亦即通过高保真面部界标来生成近似面部几何形状的网格。根据所述网格，研究人员分离出原始图像中的眼睛区域以用于虹膜追踪模型。然后，谷歌将问题分为两个部分：眼睛轮廓估计和虹膜位置。他们设计了一个由一元化编码器组成的多任务模型，每个组件对应一个任务，这样就能够使用特定于任务的训练数据。&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;http://p3.itc.cn/q_70/images03/20200808/462869a0c5844bc28f5c89f65479dd02.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;300&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定&lt;/div&gt; 
&lt;/div&gt; 
&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;为了将裁剪后的眼睛区域用于模型训练，团队手动注释了大约50万张图像。其中，图像涵盖了不同地理位置的各种照明条件和头部姿势，如下所示
&lt;div align=center&gt;
&lt;img src=&#34;d07b74a8b05a4721b72eb12c1a81f383.png&#34; alt=&#34;show&#34; width=&#34;700&#34; height=&#34;200&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定&lt;/div&gt; 
&lt;div align=center&gt;
&lt;img src=&#34;d9331ef1d8864e79a8288a210f869815.png&#34; alt=&#34;show&#34; width=&#34;700&#34; height=&#34;260&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;裁剪的眼睛区域形成模型的输入，而它将通过单独的组件预测界标&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;&lt;div align=left&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;2. 虹膜深度：用单个图像进行深度估计&lt;/font&gt;&lt;/strong&gt;
&lt;font size=&#39;5&#39;&gt;  无需任何专门的硬件，这个虹膜追踪模型能够以不到10％的误差确定对象到摄像头的度量距离。相关的原理事实是，人眼的水平直径虹膜基本恒定为11.7±0.5毫米。作为说明，请想象将针孔摄像头模型投影到正方形像素的传感器。你可以使用摄像头的焦距估计从面部界标到对象的距离，而这可以通过Camera Capture API或直接从捕获图像的EXIF元数据，以及其他摄像头固有参数进行获取。给定焦距，对象到摄像头的距离与对象眼睛的物理尺寸成正比，如下图所示&lt;/p&gt;
&lt;div align=center&gt;
&lt;img src=&#34;366c2353e9654a7998c87ac32c491daf.png&#34; alt=&#34;show&#34; width=&#34;700&#34; height=&#34;500&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;利用类似的三角形，我们可以根据焦距（f）和虹膜大小来计算对象的距离（d）&lt;/div&gt; 
&lt;div align=center&gt;
&lt;img src=&#34;http://p7.itc.cn/q_70/images03/20200808/0ba5001e03ca4c42a17036b97ed0326b.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;260&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;左边：在Pixel 2运行的MediaPipe Iris正在以cm为单位估计度量距离，没有采用任何深度摄像头；右边：groud-truth深度&lt;/div&gt; 
&lt;div align=&#39;left&#39;&gt;&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;为了量化所述方法的精确性，研究人员收集了200多位被试的正向同步视频和深度图像，并将其与iPhone 11的深度传感器进行比较。团队使用激光测距设备，通过实验确定iPhone 11的深度传感器在2米以内的误差小于2％。对于使用虹膜大小进行深度估算的方法，平均相对误差为4.3％，标准偏差是2.4％。谷歌对有眼镜被试和正常视力被试（不计入隐形眼镜情况）测试了所述方法，并发现眼镜会将平均相对误差略微提高到4.8％（标准偏差是3.1％）。另外，实验没有测试存在任何眼睛疾病的被试。考虑到MediaPipe Iris不需要专门的硬件，所述结果表明系统能够支持一系列成本范围的设备根据单张图像获取度量深度
&lt;div align=center&gt;
&lt;img src=&#34;f49af869c2c34877a244ab791ee17b27.png&#34; alt=&#34;show&#34; width=&#34;500&#34; height=&#34;420&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;估计误差的直方图（左边），以及实际和估计距离的比较（右边）&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;&lt;div align=left&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;3. 发布MediaPipe Iris&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#39;left&#39;&gt;&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;这个虹膜和深度估计模型将作为支持PC，移动设备和Web的跨平台MediaPipe管道发布。正如谷歌在最近一篇关于MediaPipe的博文所述，团队利用WebAssembly和XNNPACK在浏览器中本地运行Iris ML管道，无需将任何数据发送到云端。
&lt;div align=center&gt;
&lt;img src=&#34;http://p7.itc.cn/q_70/images03/20200808/6974ce3508e94a1c87418733ba3a3928.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;420&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;使用MediaPipe的WASM堆栈。你可以在浏览器种运行模型&lt;/div&gt; 
&lt;div align=center&gt;
&lt;img src=&#34;http://p6.itc.cn/q_70/images03/20200808/ede608441d4141629bf58a576ed495ba.gif&#34; alt=&#34;show&#34; width=&#34;300&#34; height=&#34;420&#34;，&gt;
&lt;div align=&#39;center&#39;&gt;&lt;font size=&#39;3&#39;&gt;仅使用包含EXIF数据的单张图片计算虹膜深度&lt;/div&gt; 
&lt;p&gt;&lt;strong&gt;&lt;div align=left&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;4. 未来方向&lt;/font&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;div align=&#39;left&#39;&gt;&lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;谷歌计划进一步扩展MediaPipe Iris模型，实现更稳定的追踪性能以降低误差，并将其部署用于无障碍用例。谷歌在相关文档和随附的Model Card中详细说明了预期的用途，限制和模型的公平性，从而确保模型的使用符合谷歌的AI原则。请注意，任何形式的监视监控都明显超出应用范围，故不予支持。团队表示：“我们希望的是，通过向广泛的研究与开发社区提供这种虹膜感知功能，从而促使创造性用例的出现，激发负责任的新应用和新研究途径。”</description>
    </item>
    
    <item>
      <title>How does iris recognize identity successfully?</title>
      <link>https://hycasia.github.io/post/how-does-iris-recognize-identity-successfully/</link>
      <pubDate>Wed, 05 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/post/how-does-iris-recognize-identity-successfully/</guid>
      <description>&lt;p&gt;  8月5日晚芒果台，中科院自动化研究所智能感知与计算研究中心助理研究员王云龙老师带你探索虹膜识别的奥秘&lt;/p&gt;
&lt;iframe frameborder=&#34;0&#34; width=&#34;720px&#34; height=&#34;480px&#34; src=&#34;https://www.mgtv.com/s/9538853.html&#34; allowFullScreen=&#34;true&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Academician introduces you to iris recognition</title>
      <link>https://hycasia.github.io/post/academician-introduces-you-to-iris-recognition/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/post/academician-introduces-you-to-iris-recognition/</guid>
      <description>&lt;p&gt;  7月13日播出的CCTV-1《生活圈》节目中，谭铁牛院士现身为观众介绍了虹膜识别技术。谭铁牛院士介绍到：“虹膜识别是一种相对比较新颖的生物特征识别技术，下一步虹膜识别技术会进一步朝着移动化、便捷化以及和其他的相关的生物特征识别技术，比如人脸识别技术，相融合的方向发展，具有非常广阔的发展空间。”&lt;/p&gt;
&lt;iframe frameborder=&#34;0&#34; width=&#34;720px&#34; height=&#34;480px&#34; src=&#34;20200713_17347661f3d_r29_800k.mp4&#34; allowFullScreen=&#34;true&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&amp;emsp;&amp;emsp;自动化所孙哲南研究员也携团队相关成果做客节目，具体讲解了虹膜识别技术的优势与应用。
&lt;div&gt;  
&lt;div align=&#39;center&#39;&gt;
&lt;font color=#0099ff size=6 face=&#34;黑体&#34;&gt;虹膜识别技术的优势&lt;/font&gt;
&lt;/div&gt;
&amp;emsp;&amp;emsp;虹膜识别是利用人眼表面黑色瞳孔和白色巩膜之间圆环状的区域进行身份识别的技术。虹膜识别的优势在于： 
&lt;p&gt;  第一，虹膜先天具有非常高的唯一性。虹膜中可以发现证明至少244个独立变量来决定其唯一性，而指纹和人脸大概只有十几个或者几十个这样的变量。&lt;br&gt;
  第二，虹膜终身不变。年龄的增长、化妆或者整容可以改变人的容貌，却无法改变虹膜&lt;/p&gt;
&lt;div&gt;
&lt;div align=&#39;center&#39;&gt;
&lt;font color=#0099ff size=6 face=&#34;黑体&#34;&gt;虹膜识别的应用&lt;/font&gt; 
&lt;/div&gt;  
&lt;p&gt;&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;1. 虹膜识别应用于手机&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  孙哲南研究员在节目中展示了团队研发的虹膜识别解锁手机，在手机终端装载虹膜识别模块，直接刷眼就可以解锁手机。防护镜、墨镜甚至黑暗的环境都不会成为虹膜识别的阻碍。&lt;br&gt;
&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;2. 虹膜识别应用于电脑&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  使用虹膜解锁电脑，刷眼后一瞬间即可安全登陆，省去了总是忘记密码与密码被盗的烦恼。&lt;br&gt;
&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;3. 虹膜识别防盗门锁&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  只需对准虹膜采集框，即可解锁开门。团队展示的虹膜锁采用近红外主动光源成像，即使在光线很暗的楼道内，虹膜锁也可以正常工作。&lt;br&gt;
&lt;strong&gt;&lt;font color=#000000 size=5 face=&#34;黑体&#34;&gt;4. 虹膜识别收费闸机&lt;/font&gt;&lt;/strong&gt;&lt;br&gt;
  想象一下，当我们驾车通过收费闸机时，只需要刷一下眼睛，就可以自动收费抬杆，这是一种什么样的感觉呢？将来，这一系统也可以应用于高速公路ETC中，驾驶员就可以直接通过眼神识别进行缴费。&lt;br&gt;
  &lt;strong&gt;其实，虹膜识别的应用远不止这些，并且在不远的将来，它还可以在更多地方得以运用，为人们的生活提供超乎想象的便利！&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CASIA-IrisV4</title>
      <link>https://hycasia.github.io/dataset/casia-irisv4/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/dataset/casia-irisv4/</guid>
      <description>&lt;h2 id=&#34;download-the-whole-database-186gbhttpbiometricsidealtestorgdownloaddbdoid4&#34;&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download the whole database (1.86GB)&lt;/a&gt;&lt;/h2&gt;
&lt;p style=&#34;text-align: center&#34;&gt;
OR&lt;br&gt;
Download the separated subsets below&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Interval (30.9MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Lamp (390MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Twins (60MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Distance(767MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Thousand (490MB)&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;http://biometrics.idealtest.org/downloadDB.do?id=4&amp;amp;subset=6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Download CASIA-Iris-Syn (171MB)&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;With the pronounced need for reliable personal identification, iris recognition has become an important enabling technology in our society. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. Automatic iris recognition has to face unpredictable variations of iris images in real-world applications. For example, recognition of iris images of poor quality, nonlinearly deformed iris images, iris images at a distance, iris images on the move, and faked iris images all are open problems in iris recognition. A basic work to solve the problems is to design and develop a high quality iris image database including all these variations. Moreover, a novel iris image database may help identify some frontier problems in iris recognition and leads to a new generation of iris recognition technology.&lt;/p&gt;
&lt;p&gt;CASIA Iris Image Database (CASIA-Iris) developed by our research group has been released to the international biometrics community and updated from CASIA-IrisV1 to CASIA-IrisV3 since 2002. More than 3,000 users from 70 countries or regions have downloaded CASIA-Iris and much excellent work on iris recognition has been done based on these iris image databases. Although great progress of iris recognition has been achieved since 1990s, the rapid growth of iris recognition applications has clearly highlighted two challenges, i.e. usability and scalability.&lt;/p&gt;
&lt;p&gt;Usability is the largest bottleneck of current iris recognition. It is a trend to develop long-range iris image acquisition systems for friendly user authentication. However, iris images captured at a distance are more challenging than traditional close-up iris images. Lack of long-range iris image data in the public domain has hindered the research and development of next-generation iris recognition systems.&lt;/p&gt;
&lt;p&gt;Most current iris recognition methods have been typically evaluated on medium sized iris image databases with a few hundreds of subjects. However, more and more large-scale iris recognition systems are deployed in real-world applications. Many new problems are met in classification and indexing of large-scale iris image databases. So scalability is another challenging issue in iris recognition.&lt;/p&gt;
&lt;p&gt;In order to promote research on long-range and large-scale iris recognition systems,  we are pleased to release to the public domain CASIA Iris Image Database V4.0 (or CASIA-IrisV4 for short).&lt;/p&gt;
&lt;h2 id=&#34;2-brief-descriptions-and-statistics-of-the-database&#34;&gt;2. Brief Descriptions and Statistics of the Database&lt;/h2&gt;
&lt;p&gt;CASIA-IrisV4 is an extension of CASIA-IrisV3 and contains six subsets. The three subsets from CASIA-IrisV3 are CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twins respectively. The three new subsets are CASIA-Iris-Distance, CASIA-Iris-Thousand, and CASIA-Iris-Syn.&lt;/p&gt;
&lt;p&gt;CASIA-IrisV4 contains a total of 54,601 iris images from more than 1,800 genuine subjects and 1,000 virtual subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination or synthesized. Some statistics and features of each subset are given in Table 1. The six data sets were collected or synthesized at different times and CASIA-Iris-Interval, CASIA-Iris-Lamp, CASIA-Iris-Distance, CASIA-Iris-Thousand may have a small inter-subset overlap in subjects.&lt;/p&gt;
&lt;h3 id=&#34;21--casia-iris-interval&#34;&gt;2.1  CASIA-Iris-Interval&lt;/h3&gt;
&lt;p&gt;Iris images of CASIA-Iris-Interval were captured with our self-developed close-up iris camera (Fig.1). The most compelling feature of our iris camera is that we have designed a circular NIR LED array, with suitable luminous flux for iris imaging. Because of this novel design, our iris camera can capture very clear iris images (see Fig.2). CASIA-Iris-Interval is well-suited for studying the detailed texture features of iris images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.1.jpg&#34; alt=&#34;Fig.1&#34;&gt;
Fig.1 The self-developed iris camera used for collection of CASIA-Iris-Interval
&lt;img src=&#34;./V4Fig.2.jpg&#34; alt=&#34;Fig.2&#34;&gt;
Fig.2 Example iris images in CASIA-Iris-Interval&lt;/p&gt;
&lt;h3 id=&#34;22--casia-iris-lamp&#34;&gt;2.2  CASIA-Iris-Lamp&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Lamp was collected using a hand-held iris sensor produced by OKI (Fig.3). A lamp was turned on/off close to the subject to introduce more intra-class variations when we collected CASIA-Iris-Lamp. Elastic deformation of iris texture (Fig.4) due to pupil expansion and contraction under different illumination conditions is one of the most common and challenging issues in iris recognition. So CASIA-Iris-Lamp is good for studying problems of non-linear iris normalization and robust iris feature representation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.3.jpg&#34; alt=&#34;Fig.3&#34;&gt;
Fig.3 The hand-held iris camera used for collection of CASIA-Iris-Lamp
&lt;img src=&#34;./V4Fig.4.jpg&#34; alt=&#34;Fig.4&#34;&gt;
Fig.4 Example iris images in CASIA-Iris-Lamp&lt;/p&gt;
&lt;h3 id=&#34;23--casia-iris-twins&#34;&gt;2.3  CASIA-Iris-Twins&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Twins contains iris images of 100 pairs of twins, which were collected during Annual Twins Festival in Beijing using OKI&amp;rsquo;s IRISPASS-h camera (Fig.5). Although iris is usually regarded as a kind of phenotypic biometric characteristics and even twins have their unique iris patterns, it is interesting to study the dissimilarity and similarity between iris images of twins.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.5.jpg&#34; alt=&#34;Fig.5&#34;&gt;
Fig.5 Example iris images in CASIA-Iris-Twins&lt;/p&gt;
&lt;h3 id=&#34;24--casia-iris-distance&#34;&gt;2.4  CASIA-Iris-Distance&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Distance contains iris images captured using our self-developed long-range multi-modal biometric image acquisition and recognition system (LMBS, Fig.6). The advanced biometric sensor can recognize users from 3 meters away by actively searching iris, face or palmprint patterns in the visual field via an intelligent multi-camera imaging system. The LMBS is human-oriented by fusing computer vision, human computer interaction and multi-camera coordination technologies and improves greatly the usability of current biometric systems. The iris images of CASIA-Iris-Distance were captured by a high resolution camera so both dual-eye iris and face patterns are included in the image region of interest (Fig. 7). And detailed facial features such as skin pattern are also visible for multi-modal biometric information fusion.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.6.jpg&#34; alt=&#34;Fig.6&#34;&gt;
Fig.6  The biometric sensor used for collection of CASIA-Iris-Distance
&lt;img src=&#34;./V4Fig.7.jpg&#34; alt=&#34;Fig.7&#34;&gt;
Fig.7  An example image in CASIA-Iris-Distance&lt;/p&gt;
&lt;h3 id=&#34;25--casia-iris-thousand&#34;&gt;2.5  CASIA-Iris-Thousand&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Thousand contains 20,000 iris images from 1,000 subjects, which were collected using IKEMB-100 camera (Fig. 8) produced by 
&lt;a href=&#34;Http://www.irisking.com&#34;&gt;IrisKing&lt;/a&gt;. IKEMB-100 is a dual-eye iris camera with friendly visual feedback, realizing the effect of “What You See Is What You Get”. The bounding boxes shown in the frontal LCD help users adjust their pose for high-quality iris image acquisition. The main sources of intra-class variations in CASIA-Iris-Thousand are eyeglasses and specular reflections. Since CASIA-Iris-Thousand is the first publicly available iris dataset with one thousand subjects, it is well-suited for studying the uniqueness of iris features and develop novel iris classification and indexing methods.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.8.jpg&#34; alt=&#34;Fig.8&#34;&gt;
Fig.8 The iris camera used for collection of CASIA-Iris-Thousand
&lt;img src=&#34;./V4Fig.9.jpg&#34; alt=&#34;Fig.9&#34;&gt;
Fig.9  An example image in CASIA-Iris-Thousand&lt;/p&gt;
&lt;h3 id=&#34;26--casia-iris-syn&#34;&gt;2.6  CASIA-Iris-Syn&lt;/h3&gt;
&lt;p&gt;CASIA-Iris-Syn contains 10,000 synthesized iris images of 1,000 classes. The iris textures of these images are synthesized automatically from a subset of CASIA-IrisV1 with the approach described in [1] (Fig. 10). Then the iris ring regions were embedded into the real iris images, which makes the artificial iris images more realistic. The intra-class variations introduced into the synthesized iris dataset include deformation, blurring, and rotation, which raise a challenge problem for iris feature representation and matching. We have demonstrated in [1] that the synthesized iris images are visually realistic and most subjects can not distinguish genuine and artificial iris images. More importantly, the performance results tested on the synthesized iris image database have similar statistical characteristics to genuine iris database. So users of CASIA-IrisV4 are encouraged to use CASIA-Iris-Syn for iris recognition research and any suggestions are welcome. If CASIA-Iris-Syn proves to be successful for most researchers of iris recognition, we will provide more and more synthesized iris images in the future.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;./V4Fig.10.jpg&#34; alt=&#34;Fig.10&#34;&gt;
Fig. 10  Flowchart of the iris texture synthesis method for generation of CASIA-Iris-Syn
&lt;img src=&#34;./V4Fig.11.jpg&#34; alt=&#34;Fig.11&#34;&gt;
Fig. 11  Example iris images in CASIA-Iris-Syn&lt;/p&gt;
&lt;h2 id=&#34;3-database-organization&#34;&gt;3. Database Organization&lt;/h2&gt;
&lt;p&gt;The file name of each image in CASIA-IrisV4 is unique to each other and denotes some useful properties associated with the image such as subset category, left/right/double, subject ID, class ID, image ID etc. The file naming rules of all six subsets are listed as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Interval are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Interval/YYY/S1YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Lamp are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Lamp/YYY/E/S2YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Twins are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Twins\XX\YE\S3XXYENN.jpg&lt;/p&gt;
&lt;p&gt;XX: the index of family&lt;/p&gt;
&lt;p&gt;Y: the identifier to one of the twins&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Distance are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Distance/YYY/S4YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘D’ denotes dual-eye iris image&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Thousand are stored as:&lt;/p&gt;
&lt;p&gt;$ root path$ /CASIA-Iris-Thousand/YYY/E/S5YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘L’ denotes left eye and ‘R’ denotes right eye&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The images of CASIA-Iris-Syn are stored as:&lt;/p&gt;
&lt;p&gt;root_path/CASIA-Iris-Syn/YYY/S6YYYENN.jpg&lt;/p&gt;
&lt;p&gt;YYY: the unique identifier of the subject in the subset&lt;/p&gt;
&lt;p&gt;E: ‘S’ denotes it is a synthesized iris image&lt;/p&gt;
&lt;p&gt;NN: the index of the image in the class&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;4-copyright-note-and-contacts&#34;&gt;4. Copyright Note and Contacts&lt;/h2&gt;
&lt;p&gt;The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-IrisV4 collected by the Chinese Academy of Sciences&amp;rsquo; Institute of Automation (CASIA)” and a reference to “CASIA Iris Image Database, &lt;a href=&#34;http://biometrics.idealtest.org/&#34;&gt;http://biometrics.idealtest.org/&lt;/a&gt;” should be included. A copy of all reports and papers that are for public or general release that use the CASIA-IrisV4 should be forwarded upon release or publication to:&lt;/p&gt;
&lt;p&gt;Professor Tieniu Tan&lt;/p&gt;
&lt;p&gt;Center for Biometrics and Security Research&lt;/p&gt;
&lt;p&gt;National Laboratory of Pattern Recognition&lt;/p&gt;
&lt;p&gt;Institute of Automation, Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;P.O.Box 2728&lt;/p&gt;
&lt;p&gt;Beijing 100190&lt;/p&gt;
&lt;p&gt;China&lt;/p&gt;
&lt;p&gt;or send electronic copies to &lt;a href=&#34;mailto:znsun@nlpr.ia.ac.cn&#34;&gt;znsun@nlpr.ia.ac.cn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Questions regarding this database can be addressed to Dr. Zhenan Sun at&lt;/p&gt;
&lt;p&gt;Dr. Zhenan Sun&lt;/p&gt;
&lt;p&gt;Center for Biometrics and Security Research&lt;/p&gt;
&lt;p&gt;National Laboratory of Pattern Recognition&lt;/p&gt;
&lt;p&gt;Institute of Automation, Chinese Academy of Sciences&lt;/p&gt;
&lt;p&gt;P.O.Box 2728&lt;/p&gt;
&lt;p&gt;Beijing 100190&lt;/p&gt;
&lt;p&gt;China&lt;/p&gt;
&lt;p&gt;Tel: +86 10 8261 0278&lt;/p&gt;
&lt;p&gt;Fax: +86 10 6255 1993&lt;/p&gt;
&lt;p&gt;Email: &lt;a href=&#34;mailto:znsun@nlpr.ia.ac.cn&#34;&gt;znsun@nlpr.ia.ac.cn&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;publications&#34;&gt;Publications&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Tieniu Tan, Zhaofeng He, Zhenan Sun, &amp;ldquo;Efficient and robust segmentation of noisy iris images for non-cooperative iris recognition&amp;rdquo;, Image and Vision Computing, Vol.28, No. 2, 2010, pp.223-230.&lt;/li&gt;
&lt;li&gt;T. Tan and L. Ma, “Iris Recognition: Recent Progress and Remaining Challenges”, Proc. of SPIE, Vol. 5404, pp. 183-194, 12-13 Apr 2004, Orlando, USA.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, &amp;ldquo;Ordinal Measures for Iris Recognition,&amp;rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 12, 2009, pp. 2211 - 2226.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, &amp;ldquo;Towards Accurate and Fast Iris Segmentation for Iris Biometrics&amp;rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 9, 2009, pp.1670 - 1684.&lt;/li&gt;
&lt;li&gt;L. Ma, T. Tan, Y. Wang and D. Zhang, “Personal Identification Based on Iris Texture Analysis”, IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), Vol. 25, No. 12, pp.1519-1533, 2003.&lt;/li&gt;
&lt;li&gt;Li Ma, Tieniu Tan, Yunhong Wang and Dexin Zhang, “Efficient Iris Recognition by Characterizing Key Local Variations”, IEEE Trans. on Image Processing, Vol. 13, No.6, pp. 739- 750, 2004.&lt;/li&gt;
&lt;li&gt;L. Ma, T. Tan, D. Zhang and Y. Wang, “Local Intensity Variation Analysis for Iris Recognition, Pattern Recognition”, Vol.37, No.6, pp. 1287-1298, 2004.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, IEEE Transactions on Systems, Man, and Cybernetics-Part Cï¼ŒVolume 35, Issue 3, 2005, pp.435 - 441.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, Yunhong Wang, “Robust Encoding of Local Ordinal Measures: A General Framework of Iris Recognition”, Proceedings of International Workshop on Biometric Authentication (BioAW), Lecture Notes in Computer Science, Vol.3087, 2004, pp. 270-282.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 418-425.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Robust Direction Estimation of Gradient Vector Field for Iris Recognition”, Proceedings of the 17th International Conference on Pattern Recognition, Vol.2, 2004, pp.783-786.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Cascading Statistical And Structural Classifiers For Iris Recognition”, Proceedings of IEEE International Conference on Image Processing, 2004, pp.1261-1264.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, Yunhong Wang, “Iris Recognition Based on Non-local Comparisons”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 67-77.&lt;/li&gt;
&lt;li&gt;Zhenan Sun, Tieniu Tan, and Xianchao Qiu, &amp;ldquo;Graph Matching Iris Image Blocks with Local Binary Pattern&amp;rdquo;, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 366-372.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, “Robust and Fast Assessment of Iris Image Quality”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 464 - 471.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “An Appearance-Based Method for Iris Detection”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp.1091-1096, 2004, Korea.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Junzhou Huang, Tieniu Tan, Zhenan Sun and Li Ma, “An Iris Image Synthesis Method Based on PCA and Super-Resolution”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 4, pp. 471-474, 23-26 August 2004, Cambridge, UK.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “A Fast and Robust Iris Localization Method Based on Texture Segmentation”, Proc. of SPIE, Vol. 5404, pp. 401-408, 2004, USA.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Li Ma, Tieniu Tan and Zhenan Sun, “An Iris Recognition Algorithm Using Local Extreme Points”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 442-449.&lt;/li&gt;
&lt;li&gt;Jiali Cui, Yunhong Wang, Tieniu Tan and Zhenan Sun, “Fast Recursive Mathematical Morphological Transforms”, Proc. of the 3rd International Conference on Image and Graphics (ICIG), pp. 422-425, 2004, Hong Kong.&lt;/li&gt;
&lt;li&gt;Junzhou Huang, Tieniu Tan, Li Ma, and Yunhong Wang, Phase Correlation Based Iris Image Registration Model, Journal of Computer Science and Technology, Vol.20, No.3, pp.419-425, May 2005.&lt;/li&gt;
&lt;li&gt;L. Ma, Y. Wang and T. Tan, “Iris Recognition Based on Multichannel Gabor Filtering”, Proc. of the 5th Asian Conference on Computer Vision (ACCV), Vol. I, pp.279-283, Jan 22-25, 2002, Melbourne, Australia.&lt;/li&gt;
&lt;li&gt;L. Ma, Y. Wang and T. Tan, “Iris Recognition Using Circular Symmetric Filters”, Proc. of IAPR International Conference on Pattern Recognitionï¼ˆICPRï¼‰, Vol. II, pp. 414-417, August 11-15, 2002, Quebec, Canada.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, L. Ma, T. N. Tan and Y. H. Wang, “Learning-Based Enhancement Model of Iris”, Proc. of British Machine Vision Conference (BMVC), pp. 153-162, 2003.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, L. Ma, and Y. H. Wang and T. N. Tan, “Iris Model Based on Local Orientation Description”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp. 954-959, 2004, Korea.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, Y. H. Wang, T. N. Tan and J. L. Cui, “A New Iris Segmentation Model”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 3, pp. 554-557, 23-26 August 2004, Cambridge, UK.&lt;/li&gt;
&lt;li&gt;J. Z. Huang, Y. H. Wang, J. L. Cui and T. N. Tan, “Noise Removal and Impainting Model for Iris Image”, Proc. of IEEE International Conference on Image Processing (ICIP), pp. 869-872, 2004, Singapore.&lt;/li&gt;
&lt;li&gt;Yuqing He, Yangsheng Wang and Tieniu Tan, “Iris Image Capture System Design For Personal Identification”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 546-552.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, &amp;ldquo;Robust and Fast Assessment of Iris Image quality&amp;rdquo;, Proc. of International Conference of Biometrics, pp. 464-471, 2006.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan and Zhenan Sun, &amp;ldquo;Nonlinear Iris Deformation Correction Based on Gaussian Model&amp;rdquo;, International Conference of Biometrics, pp 780-789, 2007.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Yufei Han, Zhenan Sun and Tieniu Tan, Palmprint Image Synthesis: A Preliminary Study, Proc. of IEEE International Conference on Image Processing, 2008.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Tieniu Tan and Zhenan Sun, Synthesis of Large Realistic Iris Databases Using Patch-based Sampling, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008.&lt;/li&gt;
&lt;li&gt;Zhuoshi Wei, Xianchao Qiu, Zhenan Sun and Tieniu Tan, Counterfeit Iris Detection Based on Texture Analysis, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan and Zhenan Sun, “Iris Localization via Pulling and Pushing”, Proc. of the 18th IEEE International Conference on Pattern Recognition (ICPR&#39;06), Vol.4, pp. 366-369, 2006, Hongkong.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun, Xianchao Qiu, Cheng Zhong and Wenbo Dong, Boosting Ordinal Features for Iris Recognition, Proc. of the 26th IEEE International Conference on Computer Vision and Pattern Recognition (CVPR’08) , pp. 1-8, June 23-28, Alaska, USA&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Zhenan Sun, Tieniu Tan and Xianchao Qiu, Enhanced Usability of Iris Recognition via Efficient User Interface and Iris Image Restoration, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California Accepted.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, Robust Eyelid, Eyelash and Shadow Localization for Iris Recognition”, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California, Accepted.&lt;/li&gt;
&lt;li&gt;Zhaofeng He, Tieniu Tan, Zhenan Sun and Zhuoshi Wei, “Efficient Iris Spoof Detection via Boosted Local Binary Patterns”, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1080-1090, 2009.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Coarse Iris Classification by Learned Visual Dictionary&amp;rdquo;, In Proc. of The 2nd International Conference on Biometrics, pp. 770–779, Seoul, Korea, Aug. 2007.&lt;/li&gt;
&lt;li&gt;Xianchao Qiu, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Global Texture Analysis of Iris Images for Ethnic Classification&amp;rdquo;, In Proc. of The 1st International Conference on Biometrics, pp. 411–418, Hong Kong, China. Jan. 2006.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Self-adaptive iris image acquisition system, Proc. SPIE vol. 6944, 1-9, 2008.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, How to make iris recognition easier?, Proc. of the 19th International Conference on Pattern Recognition, pp.1-4, 2008.&lt;/li&gt;
&lt;li&gt;Wenbo Dong, Zhenan Sun, Tieniu Tan, Zhuoshi Wei, &amp;ldquo;Quality-based dynamic threshold for iris matching&amp;rdquo;, In Proceedings of IEEE International Conference on Image Processing, 2009.&lt;/li&gt;
&lt;li&gt;Long Zhang, Zhenan Sun, Tieniu Tan and Shungeng Hu, &amp;ldquo;Robust Biometric Key Extraction Based on Iris Cryptosystem&amp;rdquo;, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1060-1069, 2009.&lt;/li&gt;
&lt;li&gt;Hui Zhang, Zhenan Sun, and Tieniu Tan, Contact lens detection based on weighted LBP, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.&lt;/li&gt;
&lt;li&gt;Hui Zhang, Zhenan Sun, and Tieniu Tan, Statistics of Local Surface Curvatures for Mis-Localized Iris Detection, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010.&lt;/li&gt;
&lt;li&gt;Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Texture Removal for Adaptive Level Set based Iris Segmentation&amp;rdquo;, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010.&lt;/li&gt;
&lt;li&gt;Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, &amp;ldquo;Hierarchical Fusion of Face and Iris for Personal Identification&amp;rdquo;, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition</title>
      <link>https://hycasia.github.io/publication/wang-icpr-2020/</link>
      <pubDate>Thu, 02 Jul 2020 21:57:47 +0800</pubDate>
      <guid>https://hycasia.github.io/publication/wang-icpr-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN</title>
      <link>https://hycasia.github.io/publication/wang-tci-2020/</link>
      <pubDate>Thu, 02 Jul 2020 21:36:56 +0800</pubDate>
      <guid>https://hycasia.github.io/publication/wang-tci-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iris Liveness Detection Based on Light Field Imaging</title>
      <link>https://hycasia.github.io/publication/song-automatica2019/</link>
      <pubDate>Mon, 17 Feb 2020 17:23:05 +0800</pubDate>
      <guid>https://hycasia.github.io/publication/song-automatica2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>All-in-Focus Iris Camera With a Great Capture Volume</title>
      <link>https://hycasia.github.io/publication/zhang-ijcb2020/</link>
      <pubDate>Mon, 17 Feb 2020 16:12:05 +0800</pubDate>
      <guid>https://hycasia.github.io/publication/zhang-ijcb2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recognition Oriented Iris Image Quality Assessment in the Feature Space</title>
      <link>https://hycasia.github.io/publication/wang-ijcb2020/</link>
      <pubDate>Mon, 17 Feb 2020 16:12:05 +0800</pubDate>
      <guid>https://hycasia.github.io/publication/wang-ijcb2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application</title>
      <link>https://hycasia.github.io/publication/liu-tip-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/liu-tip-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dynamic Graph Representation for Occlusion Handling in Biometrics</title>
      <link>https://hycasia.github.io/publication/ren-2020-dynamic/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/ren-2020-dynamic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation</title>
      <link>https://hycasia.github.io/publication/wang-tbiom-2020/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/wang-tbiom-2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Seg-Edge Bilateral Constraint Network for Iris Segmentation</title>
      <link>https://hycasia.github.io/publication/hu-2019-icb/</link>
      <pubDate>Wed, 08 May 2019 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/hu-2019-icb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://hycasia.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Academic&lt;/a&gt; | 
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;fragment &#34; &gt;
One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
&lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
Three
&lt;/span&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;
&lt;a href=&#34;https://spectrum.chat/academic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alignment Free and Distortion Robust Iris Recognition</title>
      <link>https://hycasia.github.io/publication/ren-2019-alignment/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/ren-2019-alignment/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cross-sensor iris recognition using adversarial strategy and sensor-specific information</title>
      <link>https://hycasia.github.io/publication/csin-btas-2019/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/csin-btas-2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution</title>
      <link>https://hycasia.github.io/publication/wang-tip-2018/</link>
      <pubDate>Sat, 01 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/wang-tip-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Hierarchical Multi-class Iris Classification for Liveness Detection</title>
      <link>https://hycasia.github.io/publication/yan-2018-liveness/</link>
      <pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/yan-2018-liveness/</guid>
      <description></description>
    </item>
    
    <item>
      <title>End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN</title>
      <link>https://hycasia.github.io/publication/wang-eccv-2018/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/wang-eccv-2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation</title>
      <link>https://hycasia.github.io/publication/ren-ccbr-2017/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/ren-ccbr-2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iris recognition is the general trend?</title>
      <link>https://hycasia.github.io/post/iris-recognition-is-the-general-trend/</link>
      <pubDate>Tue, 18 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/post/iris-recognition-is-the-general-trend/</guid>
      <description> &lt;font size=&#39;5&#39;&gt;
 &amp;emsp;&amp;emsp;2017年4月27日，微软获得了一项虹膜识别技术的专利，未来该技术将纳入 Windows Hello ，用于微软旗下的智能手机、笔记本等设备中。
 &lt;/font&gt;
&lt;div align=center&gt;
&lt;img src=&#34;https://blog-assets.oss-cn-shanghai.aliyuncs.com/530/4db4abc1a1c5a9ef9687e58441c419544ca22a98.gif&#34; alt=&#34;show&#34; /&gt;
&lt;/div&gt;
 &lt;font size=&#39;5&#39;&gt;
&amp;emsp;&amp;emsp;虹膜识别是生物识别技术中的一种。其他的生物识别方法包括人脸、指纹、声音、视网膜、静脉识别等，而由于人类虹膜上拥有266个特征点，远高于其他生物识别技术的不到60个特征点，故被认为具有更高的精准性和安全性。
&lt;div&gt;
&lt;p&gt;&lt;font size=&#39;5&#39;&gt;  虹膜识别是通过数学算法对人眼虹膜特征进行编码和对比的身份识别方法。根据专利文件描述，微软的智能设备可以从两个或者三个方向照明中拍摄用户眼睛的多张照片。每个角度的眼睛照片都能检测虹膜特征并创建不同的数据点。&lt;/p&gt;
 &lt;div align=center&gt;
&lt;img src=&#34;15d24fe272954868ba8649893ea5ad34633d843f.jpeg&#34; alt=&#34;show&#34; /&gt;
&lt;/div&gt;
 &lt;font size=&#39;5&#39;&gt;&amp;emsp;&amp;emsp;微软在其专利申请中指出，人眼是部分透明的三维结构。当光通过瞳孔传递到眼睛的视网膜上。从不同的方向用光照射眼睛，就可以获得许多图像帧的图像数据，并对至少两个图像帧的数据进行对比，找到相似的地方，获得相关的数据，这些数据与关注的眼睛区域是一致的。然后系统根据数据自动确定假眼睛的验证特点，从而用来验证真正的眼睛。
</description>
    </item>
    
    <item>
      <title>A simple and robust super resolution method for light field images</title>
      <link>https://hycasia.github.io/publication/wang-icip-2016/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/wang-icip-2016/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iris Recognition</title>
      <link>https://hycasia.github.io/benchmark/iris-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/benchmark/iris-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Light Field Photography</title>
      <link>https://hycasia.github.io/benchmark/light-field-photography/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/benchmark/light-field-photography/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Light Field Photography</title>
      <link>https://hycasia.github.io/project/light-field-photography/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/project/light-field-photography/</guid>
      <description>&lt;h2 id=&#34;light-field&#34;&gt;Light field&lt;/h2&gt;
&lt;p&gt;The light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled &amp;ldquo;Thoughts on Ray Vibrations&amp;rdquo;) that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase light field was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936).&lt;/p&gt;
&lt;h2 id=&#34;the-4d-light-field&#34;&gt;The 4D light field&lt;/h2&gt;
&lt;p&gt;In a plenoptic function, if the region of interest contains a concave object (think of a cupped hand), then light leaving one point on the object may travel only a short distance before being blocked by another point on the object. No practical device could measure the function in such a region.&lt;/p&gt;
&lt;p&gt;However, if we restrict ourselves to locations outside the convex hull (think shrink-wrap) of the object, i.e. in free space, then we can measure the plenoptic function by taking many photos using a digital camera. Moreover, in this case the function contains redundant information, because the radiance along a ray remains constant from point to point along its length, as shown at left. In fact, the redundant information is exactly one dimension, leaving us with a four-dimensional function (that is, a function of points in a particular four-dimensional manifold). Parry Moon dubbed this function the photic field (1981), while researchers in computer graphics call it the 4D light field (Levoy 1996) or Lumigraph (Gortler 1996). Formally, the 4D light field is defined as radiance along rays in empty space.&lt;/p&gt;
&lt;p&gt;The set of rays in a light field can be parameterized in a variety of ways, a few of which are shown below. Of these, the most common is the two-plane parameterization shown at right (below). While this parameterization cannot represent all rays, for example rays parallel to the two planes if the planes are parallel to each other, it has the advantage of relating closely to the analytic geometry of perspective imaging. Indeed, a simple way to think about a two-plane light field is as a collection of perspective images of the st plane (and any objects that may lie astride or beyond it), each taken from an observer position on the uv plane. A light field parameterized this way is sometimes called a light slab.&lt;/p&gt;
&lt;h2 id=&#34;ways-to-create-light-fields&#34;&gt;Ways to create light fields&lt;/h2&gt;
&lt;p&gt;Light fields are a fundamental representation for light. As such, there are as many ways of creating light fields as there are computer programs capable of creating images or instruments capable of capturing them.&lt;/p&gt;
&lt;p&gt;In computer graphics, light fields are typically produced either by rendering a 3D model or by photographing a real scene. In either case, to produce a light field views must be obtained for a large collection of viewpoints. Depending on the parameterization employed, this collection will typically span some portion of a line, circle, plane, sphere, or other shape, although unstructured collections of viewpoints are also possible (Buehler 2001).&lt;/p&gt;
&lt;p&gt;Devices for capturing light fields photographically may include a moving handheld camera or a robotically controlled camera (Levoy 2002), an arc of cameras (as in the bullet time effect used in The Matrix), a dense array of cameras (Kanade 1998; Yang 2002; Wilburn 2005), handheld cameras (Ng 2005; Georgiev 2006; Marwah 2013), microscopes (Levoy 2006), or other optical system (Bolles 1987).&lt;/p&gt;
&lt;p&gt;How many images should be in a light field? The largest known light field (of Michelangelo&amp;rsquo;s statue of Night) contains 24,000 1.3-megapixel images. At a deeper level, the answer depends on the application. For light field rendering (see the Application section below), if you want to walk completely around an opaque object, then of course you need to photograph its back side. Less obviously, if you want to walk close to the object, and the object lies astride the st plane, then you need images taken at finely spaced positions on the uv plane (in the two-plane parameterization shown above), which is now behind you, and these images need to have high spatial resolution.&lt;/p&gt;
&lt;p&gt;The number and arrangement of images in a light field, and the resolution of each image, are together called the &amp;ldquo;sampling&amp;rdquo; of the 4D light field. Analyses of light field sampling have been undertaken by many researchers; a good starting point is Chai (2000). Also of interest is Durand (2005) for the effects of occlusion, Ramamoorthi (2006) for the effects of lighting and reflection, and Ng (2005) and Zwicker (2006) for applications to plenoptic cameras and 3D displays, respectively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Periocular Recognition</title>
      <link>https://hycasia.github.io/benchmark/periocular-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/benchmark/periocular-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Periocular Recognition</title>
      <link>https://hycasia.github.io/project/periocular-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/project/periocular-recognition/</guid>
      <description>&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 7 &gt;&lt;strong&gt;To be updated.....&lt;/strong&gt;&lt;/font&gt;</description>
    </item>
    
    <item>
      <title>Sclera Recognition</title>
      <link>https://hycasia.github.io/benchmark/sclera-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/benchmark/sclera-recognition/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Sclera Recognition</title>
      <link>https://hycasia.github.io/project/sclera-recognition/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/project/sclera-recognition/</guid>
      <description>&lt;div style=&#34;width:800px;&#34;&gt;&lt;font color=#000000 size = 7 &gt;&lt;strong&gt;To be updated.....&lt;/strong&gt;&lt;/font&gt;</description>
    </item>
    
    <item>
      <title>The Other Internal Project</title>
      <link>https://hycasia.github.io/benchmark/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/benchmark/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4D light-field sensing system for people counting</title>
      <link>https://hycasia.github.io/publication/hou-spie-2016/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/hou-spie-2016/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
