<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Light Field Photography | SIR-Smart Iris Recognition</title>
    <link>https://hycasia.github.io/tag/light-field-photography/</link>
      <atom:link href="https://hycasia.github.io/tag/light-field-photography/index.xml" rel="self" type="application/rss+xml" />
    <description>Light Field Photography</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 27 Apr 2016 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hycasia.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_2.png</url>
      <title>Light Field Photography</title>
      <link>https://hycasia.github.io/tag/light-field-photography/</link>
    </image>
    
    <item>
      <title>Light Field Photography</title>
      <link>https://hycasia.github.io/project/light-field-photography/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/project/light-field-photography/</guid>
      <description>&lt;h2 id=&#34;light-field&#34;&gt;Light field&lt;/h2&gt;
&lt;p&gt;The light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled &amp;ldquo;Thoughts on Ray Vibrations&amp;rdquo;) that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase light field was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936).&lt;/p&gt;
&lt;h2 id=&#34;the-4d-light-field&#34;&gt;The 4D light field&lt;/h2&gt;
&lt;p&gt;In a plenoptic function, if the region of interest contains a concave object (think of a cupped hand), then light leaving one point on the object may travel only a short distance before being blocked by another point on the object. No practical device could measure the function in such a region.&lt;/p&gt;
&lt;p&gt;However, if we restrict ourselves to locations outside the convex hull (think shrink-wrap) of the object, i.e. in free space, then we can measure the plenoptic function by taking many photos using a digital camera. Moreover, in this case the function contains redundant information, because the radiance along a ray remains constant from point to point along its length, as shown at left. In fact, the redundant information is exactly one dimension, leaving us with a four-dimensional function (that is, a function of points in a particular four-dimensional manifold). Parry Moon dubbed this function the photic field (1981), while researchers in computer graphics call it the 4D light field (Levoy 1996) or Lumigraph (Gortler 1996). Formally, the 4D light field is defined as radiance along rays in empty space.&lt;/p&gt;
&lt;p&gt;The set of rays in a light field can be parameterized in a variety of ways, a few of which are shown below. Of these, the most common is the two-plane parameterization shown at right (below). While this parameterization cannot represent all rays, for example rays parallel to the two planes if the planes are parallel to each other, it has the advantage of relating closely to the analytic geometry of perspective imaging. Indeed, a simple way to think about a two-plane light field is as a collection of perspective images of the st plane (and any objects that may lie astride or beyond it), each taken from an observer position on the uv plane. A light field parameterized this way is sometimes called a light slab.&lt;/p&gt;
&lt;h2 id=&#34;ways-to-create-light-fields&#34;&gt;Ways to create light fields&lt;/h2&gt;
&lt;p&gt;Light fields are a fundamental representation for light. As such, there are as many ways of creating light fields as there are computer programs capable of creating images or instruments capable of capturing them.&lt;/p&gt;
&lt;p&gt;In computer graphics, light fields are typically produced either by rendering a 3D model or by photographing a real scene. In either case, to produce a light field views must be obtained for a large collection of viewpoints. Depending on the parameterization employed, this collection will typically span some portion of a line, circle, plane, sphere, or other shape, although unstructured collections of viewpoints are also possible (Buehler 2001).&lt;/p&gt;
&lt;p&gt;Devices for capturing light fields photographically may include a moving handheld camera or a robotically controlled camera (Levoy 2002), an arc of cameras (as in the bullet time effect used in The Matrix), a dense array of cameras (Kanade 1998; Yang 2002; Wilburn 2005), handheld cameras (Ng 2005; Georgiev 2006; Marwah 2013), microscopes (Levoy 2006), or other optical system (Bolles 1987).&lt;/p&gt;
&lt;p&gt;How many images should be in a light field? The largest known light field (of Michelangelo&amp;rsquo;s statue of Night) contains 24,000 1.3-megapixel images. At a deeper level, the answer depends on the application. For light field rendering (see the Application section below), if you want to walk completely around an opaque object, then of course you need to photograph its back side. Less obviously, if you want to walk close to the object, and the object lies astride the st plane, then you need images taken at finely spaced positions on the uv plane (in the two-plane parameterization shown above), which is now behind you, and these images need to have high spatial resolution.&lt;/p&gt;
&lt;p&gt;The number and arrangement of images in a light field, and the resolution of each image, are together called the &amp;ldquo;sampling&amp;rdquo; of the 4D light field. Analyses of light field sampling have been undertaken by many researchers; a good starting point is Chai (2000). Also of interest is Durand (2005) for the effects of occlusion, Ramamoorthi (2006) for the effects of lighting and reflection, and Ng (2005) and Zwicker (2006) for applications to plenoptic cameras and 3D displays, respectively.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>4D light-field sensing system for people counting</title>
      <link>https://hycasia.github.io/publication/hou-spie-2016/</link>
      <pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate>
      <guid>https://hycasia.github.io/publication/hou-spie-2016/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
