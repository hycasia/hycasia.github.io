[{"authors":["Caiyong Wang"],"categories":null,"content":"Caiyong Wang is currently a Lecturer with the School of Electrical and Information Engineering, Beijing University of Civil Engineering and Architecture, China. He successfully received the Ph.D. degree in Pattern Recognition and Intelligent Systems from Institute of Automation, Chinese Academy of Sciences (CASIA), China, in 2020 (supervised by Prof. Zhenan Sun). He received the B.E. degree in applied mathematics from Xinjiang University, China, in 2013, and the M.S. degree in computational mathematics from Xiamen University, China, in 2016. His research interests include biometrics, computer vision, and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2dbc393cff8e8ff4bbce641bbcac1161","permalink":"https://hycasia.github.io/author/caiyong-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/caiyong-wang/","section":"authors","summary":"Caiyong Wang is currently a Lecturer with the School of Electrical and Information Engineering, Beijing University of Civil Engineering and Architecture, China. He successfully received the Ph.D. degree in Pattern Recognition and Intelligent Systems from Institute of Automation, Chinese Academy of Sciences (CASIA), China, in 2020 (supervised by Prof.","tags":null,"title":"Caiyong Wang","type":"authors"},{"authors":["Hongda Liu"],"categories":null,"content":"Hongda Liu is currently pursuing the Ph.D. degree with Center for Research on Intelligent Perception and Computing (CRIPAC),National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.S. degree in college of computer science and technology, Jilin University, China, in 2020. His research focuses on biometrics, computer vision, and deep learning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5796074cb306c3f5f5a81d5934c966aa","permalink":"https://hycasia.github.io/author/hongda-liu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/hongda-liu/","section":"authors","summary":"Hongda Liu is currently pursuing the Ph.D. degree with Center for Research on Intelligent Perception and Computing (CRIPAC),National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Hongda Liu","type":"authors"},{"authors":["Jianze Wei"],"categories":null,"content":"Jianze Wei is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. and M.S. degrees in communication engineering from the Civil Aviation University of China, Tianjin, China, in 2015 and 2018, respectively. His current research interests include biometrics, machine learning and computer vision.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6562cb9785fb6ba4e80235ccd2944d72","permalink":"https://hycasia.github.io/author/jianze-wei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/jianze-wei/","section":"authors","summary":"Jianze Wei is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Jianze Wei","type":"authors"},{"authors":["Junxing Hu"],"categories":null,"content":"Junxing Hu is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. degree in software engineering from the Dalian University of Technology (DUT), China, in 2017, and the M.S. degree in computer science from the Institute of Software, Chinese Academy of Sciences (ISCAS), China, in 2020. His current research interests include biometrics, computer vision, and deep leaning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"daf756d7d9916ef86df32c113688593f","permalink":"https://hycasia.github.io/author/junxing-hu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/junxing-hu/","section":"authors","summary":"Junxing Hu is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Junxing Hu","type":"authors"},{"authors":["Kunbo Zhang"],"categories":null,"content":"Kunbo Zhang is currnetly an Assistant Professor at CRIPAC, NLPR, CASIA, China. He received the B.E. degree in Automation from Beijing Institute of Technology in 2006, and the M.Sc. and Ph.D. degrees in Mechanical Engineering from State University of New York at Stony Brook, U.S., in 2008 and 2011, repectively. Between 2011 and 2016 he worked as a machine vision engineer of Advanced Manufacturing Engineering group in Nexteer Automotive, Michigan, U.S.. His current resserch interests focus on light-field photography, biometric imaging, robot vision, human-robot interaction, and intelligent manufacturing.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"571b00afbac8a6388ea006ab6babc12d","permalink":"https://hycasia.github.io/author/kunbo-zhang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/kunbo-zhang/","section":"authors","summary":"Kunbo Zhang is currnetly an Assistant Professor at CRIPAC, NLPR, CASIA, China. He received the B.E. degree in Automation from Beijing Institute of Technology in 2006, and the M.Sc. and Ph.","tags":null,"title":"Kunbo Zhang","type":"authors"},{"authors":["Leyuan Wang"],"categories":null,"content":"Leyuan Wang is currently pursuing the Master degree with Center for Research on Intelligent Perception and Computing (CRIPAC),National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. degree from Department of Forensic Audio and Video Technology, Criminal Investigation Police University of China in 2018. His research focuses on pattern recognition, Computational Photography, Biometrics and Criminal Law.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"498111336a79b02d59baa003a4fe4d27","permalink":"https://hycasia.github.io/author/leyuan-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/leyuan-wang/","section":"authors","summary":"Leyuan Wang is currently pursuing the Master degree with Center for Research on Intelligent Perception and Computing (CRIPAC),National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Leyuan Wang","type":"authors"},{"authors":["Min Ren"],"categories":null,"content":"Min Ren is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received his B.E. degree in mechanical engineering and automation from National University of Defense Technology, China, in 2013. His research focuses on pattern recognition and biometrics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6b0177bf3e3641546dd08edd5d3b5858","permalink":"https://hycasia.github.io/author/min-ren/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/min-ren/","section":"authors","summary":"Min Ren is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Min Ren","type":"authors"},{"authors":["Tao Qin"],"categories":null,"content":"Tao Qin is currently pursuing the Master. degree with the Hunan University of Technology(HUT). He received B.E. degree in Automation from the School of Mechanical and Electrical Engineering at Zhoukou Normal University. His research focuses on pattern recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"4a8270a272e9fe6cde36a754db74479a","permalink":"https://hycasia.github.io/author/tao-qin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tao-qin/","section":"authors","summary":"Tao Qin is currently pursuing the Master. degree with the Hunan University of Technology(HUT). He received B.E. degree in Automation from the School of Mechanical and Electrical Engineering at Zhoukou Normal University.","tags":null,"title":"Tao Qin","type":"authors"},{"authors":["Tianhao Lu"],"categories":null,"content":"Tianhao Lu is currently pursuing the Master. degree with the Hunan University of Technology(HUT). He received B.E. degree in Beijing Jiaotong University. He is now studying for a master\u0026rsquo;s degree in Hunan University of Technology. His research focuses on pattern recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"dfbd4d629b89375f297ef2c4cc336568","permalink":"https://hycasia.github.io/author/tianhao-lu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tianhao-lu/","section":"authors","summary":"Tianhao Lu is currently pursuing the Master. degree with the Hunan University of Technology(HUT). He received B.E. degree in Beijing Jiaotong University. He is now studying for a master\u0026rsquo;s degree in Hunan University of Technology.","tags":null,"title":"Tianhao Lu","type":"authors"},{"authors":["Tieniu Tan"],"categories":null,"content":"Tieniu Tan received the B.Sc. degree in electronic engineering from Xi’an Jiaotong University, China, in 1984, and the M.Sc. and Ph.D. degrees in electronic engineering from Imperial College London, U.K., in 1986 and 1989, respectively. He is currently a Professor with the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA). His current research interests include biometrics, image and video understanding, information hiding, and information forensics. He is a Fellow of IEEE and the IAPR (International Association of Pattern Recognition).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"99b8d6894d308cad9c2a786ee12cd352","permalink":"https://hycasia.github.io/author/tieniu-tan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tieniu-tan/","section":"authors","summary":"Tieniu Tan received the B.Sc. degree in electronic engineering from Xi’an Jiaotong University, China, in 1984, and the M.Sc. and Ph.D. degrees in electronic engineering from Imperial College London, U.K., in 1986 and 1989, respectively.","tags":null,"title":"Tieniu Tan","type":"authors"},{"authors":["Wanting Zhou"],"categories":null,"content":"Wanting Zhou is currently a postdoc with the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. She received the B.E. degree in Automation Technology, the M.S. degree in Control Engineering and the Ph.D. degree in Detection Technology and Automation Devices from North China Electric Power University, Beijing, China in 2014, 2016 and 2019. She was funded by the China Scholarship Council and did some research as a joint PhD. student in the University of Cambridge in 2018. She received the National Postdoctoral Program for Innovative Talents from China Postdoctoral Science Foundation in 2019. Her current research interests include biometrics, computer vision, and deep leaning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"11dcc0c8ae6cdc5b962dcbf0a89d8887","permalink":"https://hycasia.github.io/author/wanting-zhou/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/wanting-zhou/","section":"authors","summary":"Wanting Zhou is currently a postdoc with the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Wanting Zhou","type":"authors"},{"authors":["Yong He"],"categories":null,"content":"Yong He is is currently an Engineer with Artificial Intelligence, University of Chinese Academy of Sciences, China, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA). He received his M.D. degree in Computer Architect from Hunan University of Technology, China, in 2020. His research focuses on pattern recognition and biometrics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1b7e2599977c3660480c3c43c0f9442f","permalink":"https://hycasia.github.io/author/yong-he/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yong-he/","section":"authors","summary":"Yong He is is currently an Engineer with Artificial Intelligence, University of Chinese Academy of Sciences, China, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA).","tags":null,"title":"Yong He","type":"authors"},{"authors":["Yunlong Wang"],"categories":null,"content":"Yunlong Wang is currently an Assistant Professor with CRIPAC, NLPR, CASIA, China. He received the B.E. degree and the M.S. degree in Department of Automation, University of Science and Technology of China. His reserach focuses on pattern recognition, machine learning, light field photography, and biometrics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d707692c1920926e70fb31a5d08cf28e","permalink":"https://hycasia.github.io/author/yunlong-wang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yunlong-wang/","section":"authors","summary":"Yunlong Wang is currently an Assistant Professor with CRIPAC, NLPR, CASIA, China. He received the B.E. degree and the M.S. degree in Department of Automation, University of Science and Technology of China.","tags":null,"title":"Yunlong Wang","type":"authors"},{"authors":["Zhenan Sun"],"categories":null,"content":"Zhenan Sun is currently an Professor with CRIPAC, NLPR, CASIA, China. He received the B.E. degree in industrial automation from Dalian University of Thchnology, and the M.S. degree in system engineering from Huazhong University of Science and Technology, and the Ph.D. degree in pattern recognition and intelligent system from CASIA in 1999, 2002, and 2006 respectively. His research focuses on biometrics and pattern recognition.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"51cdf962e6cea34e169b1e05383e506e","permalink":"https://hycasia.github.io/author/zhenan-sun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhenan-sun/","section":"authors","summary":"Zhenan Sun is currently an Professor with CRIPAC, NLPR, CASIA, China. He received the B.E. degree in industrial automation from Dalian University of Thchnology, and the M.S. degree in system engineering from Huazhong University of Science and Technology, and the Ph.","tags":null,"title":"Zhenan Sun","type":"authors"},{"authors":["Zhengquan Luo"],"categories":null,"content":"Zhengquan Luo is currently a Ph.D. candidate with Department of Automation, University of Science and Technology of China(USTC), and also with Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.E. degree majoring in automation, University of Science and Technology of China(USTC), China in 2018.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"63da2a2730da1d919f264a90bcd12076","permalink":"https://hycasia.github.io/author/zhengquan-luo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zhengquan-luo/","section":"authors","summary":"Zhengquan Luo is currently a Ph.D. candidate with Department of Automation, University of Science and Technology of China(USTC), and also with Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received the B.","tags":null,"title":"Zhengquan Luo","type":"authors"},{"authors":["Zihui Yan"],"categories":null,"content":"Zihun Yan is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China. He received his B.E. degree in automation from Tsinghua University, China, in 2015. His research focuses on pattern recognition and biometrics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"194ed2105ad8d80c82adf41c08b2355e","permalink":"https://hycasia.github.io/author/zihui-yan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/zihui-yan/","section":"authors","summary":"Zihun Yan is currently pursuing the Ph.D. degree with the School of Artificial Intelligence, University of Chinese Academy of Sciences, and the Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA), China.","tags":null,"title":"Zihui Yan","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://hycasia.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://hycasia.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://hycasia.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://hycasia.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":" \u0026emsp;\u0026emsp;虹膜识别是进一步提升安全性、可靠性的必由之路 \u0026emsp;\u0026emsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;  \u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;———谭铁牛院士 \u0026emsp;\u0026emsp;虹膜识别是最具潜力的生物识别方法之一，是识别率高、非接触性、防欺骗性好的识别方法。虹膜属于人眼的一部分，如下图所示。  \u0026emsp;\u0026emsp;人眼的外观主要由巩膜、虹膜、瞳孔三部分组成，其中巩膜即眼球外围的白色部分，约占人眼总面积的30%，眼睛的中心为瞳孔部分，约占5%，虹膜位于巩膜和瞳孔之间，约占整个眼睛的65%，包含了最丰富的纹理信息，外观上看，虹膜由许多腺窝、褶皱、色素斑等构成，是人体中最独特的结构之一。因为瞳孔、虹膜和巩膜一般颜色不同，灰度值呈梯度变化，所以根据它们灰度不同，可以将它们明显分开。从几何形状可以看出，虹膜的内、外边界可以近似为圆形，这使它具有易检测性。临床观察发现：虹膜在人的一生当中几乎不发生变化，只有很少的虹膜纹理可能会由于年龄或者外伤导致纹理破坏。 \u0026emsp;\u0026emsp;作为表示个人身份的标识物，必须具备作为身份标识的重要特征。人脸、指纹等许多生物特征具有作为身份标识的特性，但是，虹膜在这些特性方面表现的更为突出，具有许多先天优势，是其他生物特征无法与之媲美的。 普遍性————虹膜是每个人天生都具有的。唯一性————虹膜的纤维组织细节复杂而丰富，每个人错综复杂的虹膜独一无二，只与虹膜的形成过程有关。稳定性————虹膜从婴儿胚胎发育的第三个月起开始发育，到第八个月虹膜的主要纹理结构已经成形。非入侵检测————从一定距离即可获得虹膜数字图像，无需用户接触设备。可接受程度好————虹膜识别以其认证准确度高、速度快、安全性高，被用户所接受。可检测性————利用图像处理技术检测出虹膜边界，易于拟合分割和和归一化防伪性高————虹膜的半径小，在可见光下中国人的虹膜图像呈现深褐色，看不到纹理信息，需要虹膜图像专业采集设备和用户的配合，所以一般情况下很难被盗取防欺骗性好————虹膜的唯一性决定了不同人眼的虹膜很难被冒充模仿。 \u0026emsp;\u0026emsp;生物特征识别通过捕获生物样本，然后采用数学方法把样本转化成相同大小的模板，提取有效的可区别性特征，就可以客观地和其他一个完整的虹膜身份识别系统主要由四个部分组成：虹膜图像获取、虹膜图像预处理、虹膜特征提取、模式比对。  虹膜图像获取 \u0026emsp;虹膜图像采集的目的是为了获取有效的虹膜图像，在传统的虹膜识别场景中，通常采用专业的成像装置在近红外光（波长700nm到900nm）照射和用户的配合下才能捕获清晰的高分辨率虹膜图像。近些年来，随着光学镜头、传感器和计算成像技术等的发展，虹膜识别的可用距离不断变大，相关装置也变得越来越轻巧实用，“远距离”、“行进中”、“移动端”和“可见光下”等少约束场景的虹膜识别对于用户使用时的约束越来越少，极大地提升了虹膜识别应用范围和用户友好性。 虹膜图像预处理 虹膜图像除了必须的虹膜区域以外，也包含了诸如巩膜、睫毛、瞳孔等非虹膜区域，因此不能直接用于虹膜识别。其次，一些噪声茹照明变化、睫毛遮挡、镜面反射、瞳孔放缩等会明显增加虹膜的类内差异，降低虹膜的识别率。常规的虹膜预处理步骤包括：虹膜活体检测、虹膜图像质量评估、虹膜分割、虹膜归一化和虹膜图像增强。 虹膜图像特征分析 虹膜图像特征分析主要包含两部分：特征提取和对比分类。虹膜特征提取是指从归一化的虹膜图像中提取紧凑有区分的虹膜特征，然后使用计算机可以存储和读取的格式进行编码。虹膜比对和分类(或者称为匹配)是指将提取的虹膜特征编码和事先在数据库注册过的虹膜特征编码通过某种相似性度量比如汉明距离、余弦距离进行对比，计算相似性分数，依次确定用户身份。  ","date":1599004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599004800,"objectID":"ceaaad5587e16c03d7ce4d8b845013a6","permalink":"https://hycasia.github.io/project/iris-recognition/","publishdate":"2020-09-02T00:00:00Z","relpermalink":"/project/iris-recognition/","section":"project","summary":"Iris recognition is considered to be the safest and most accurate recognition method","tags":["Iris Recognition"],"title":"Iris Recognition","type":"project"},{"authors":null,"categories":null,"content":"Introduction Iris is considered one of the most accurate and reliable biometric modality. Iris is more stable and distinctive compared with fingerprint, face, voice, etc, and difficult to be replicated for spoof attacks. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. In practical applications, the iris recognition system must face various unpredictable iris image degraded. For example, recognition of low-quality iris images, non-cooperative iris images, long-range iris images, and moving iris images are all huge problems in iris recognition. We believe that the first step in solving these problems is to design and develop a database of iris images that includes all of these degraded.\nBrief Descriptions and Statistics of the Database CASIA-Iris-Complex contains totally 22,932 images from 292 Asian subjects. It includes two subsets: CASIA-Iris-CX1 and CASIA-Iris-CX2. All images were collected under NIR illumination and two eyes were captured simultaneously. Detailed information of each subset is shown below.\n   Subset Characteristics CASIA-Iris-CX1 CASIA-Iris-CX2     Sensor Canon EOS 1300D MindVision MV-SUF1200GM-T   Environment Indoor with brightness change Indoor   No. of subjects 255 37   No. of classes 510 73   No. of images 18785 4147   Resolution 4608*3456 640*480   Format jpeg bmp   Distance 0.75m 1m,3m,5m   Object Partial face Eye    CASIA-Iris-CX1 CASIA-Iris-CX1 is designed to explore quality degrade caused by subject such as pupil dilation, strabismus, and occlusion. Partial face images were captured with a modified Canon EOS 1300D camera which Infrared cut-off filters was replaced by Infrared passing filters. All collection work was finished indoors, and both visible light source and near-infrared light source are used for illumination. For pupil dilation, we changed the intensity of visible light to stimulate pupil variety while keeping the intensity of infrared light unchanged. In addition, we also collected images under natural light, such as noon, dusk, sunny and cloudy. For strabismus, we placed five targets behind the camera, respectively at the left, top left, top, top right, and right positions. Looking at these targets will cause a squint of about 45 degrees. For occlusion, Volunteers were required to wear glasses, mask and using hand to cover mouth. During this section, the near-infrared light source is randomly moved to generate light spot. Meanwhile some glasses also have stains on the surface to occlude iris. Another type of occlusion comes from the person being collected. Volunteers were required to squint or close their eyes. We noticed a more interesting problem is that for some elderly volunteers, their eyelids will naturally sag, resulting in very serious occlusion. Furthermore, both defocus and motion blur are inevitable during the acquisition process, and we retain these images for research on related issues.\nCASIA-Iris-CX2 CASIA-Iris-CX2 is a small-scale experimental dataset used to explore the problems in long-distance iris recognition. We use a self-developed iris imaging system, which can obtain iris images with sufficient resolution (diameter greater than 60 pixels) in the range of 1-6m. In the case of a long distance, the image may be defocus blurred due to the error of distance perception. Therefore, we collected a continuous zoom image sequence which is changed as \u0026ldquo;blur-clear-blur“.\nAnother problem is near-infrared lighting. The illuminance of a light source with a fixed power and a fixed position is different at different distances. In order to ensure the image consistency of the entire subdataset, we moved the near-infrared light source during the acquisition process and ensured that the brightness of the image was basically equal. At the same time, we also obtained a series of over-exposed/under-exposed images by varying the exposure time (8ms ~ 35ms). Database Organization The file name of each image in CASIA-Iris-Complex is unique to each other and denotes some useful properties associated with the image such as subject ID, left/right eye, image ID etc.\n  The images of CASIA-Iris-CX1 are stored as: AAAA_B_Y_Y_CD_DDD.jpg\n AAAA: the unique identifier of the subject in the subset. B: \u0026lsquo;1\u0026rsquo; denotes left eye and \u0026lsquo;2\u0026rsquo; denotes right eye. CC: represent the acquisition conditions.  \u0026lsquo;2x\u0026rsquo; represents brightness, where x is between 0-4, means \u0026lsquo;natural light\u0026rsquo;, \u0026lsquo;dark\u0026rsquo;, \u0026lsquo;weak\u0026rsquo;, \u0026lsquo;medium\u0026rsquo;, and \u0026lsquo;strong\u0026rsquo; brightness. \u0026lsquo;3x\u0026rsquo; represents the direction of thr person looks at, where x is between 1-4, means left-up, right-up, right-down, left-down. \u0026lsquo;40\u0026rsquo; and \u0026lsquo;41\u0026rsquo; represents squinting and closing eyes. \u0026lsquo;5x\u0026rsquo; represents occlusion, where x is the occlusion type, and the value is between 1-3, which in turn means that the hand covers the face, wears a mask, and wears glasses Note: Unless otherwise specified, the brightness of the image is \u0026ldquo;medium\u0026rdquo;, the subject is required to look directly at the camera and not wear obstructions such as glasses.   DDD: the index of the image in the same scenarios    The images of CASIA-Iris-CX1 are stored as: AAAA_B_CD_TTTTTTTTTT.jpg\n AAAA: the unique identifier of the subject in the subset. B: \u0026lsquo;1\u0026rsquo; denotes left eye and \u0026lsquo;2\u0026rsquo; denotes right eye. C: the acquisition distance, and the value is between 1-5. D: the acquisition conditions.  \u0026lsquo;1\u0026rsquo; denotes normal; \u0026lsquo;2\u0026rsquo; denotes lens zoom; \u0026lsquo;3\u0026rsquo; denotes exposure time changes \u0026lsquo;4\u0026rsquo; denotes turning eyes; \u0026lsquo;5\u0026rsquo; denotes turning head;   TTTTTTTTTT: the timestamp of this image.    Copyright Note and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as \u0026ldquo;Portions of the research in this paper use the CASIA-Iris-Complex-V1.0 collected by the Chinese Academy of Sciences\u0026rsquo; Institute of Automation (CASIA)\u0026rdquo;.\nTo download this dataset, plase contact sir@cripac.ia.ac.cn\n","date":1598918400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598918400,"objectID":"4e202c4a62eeac42e588191887953d04","permalink":"https://hycasia.github.io/dataset/casia-iris-complex/","publishdate":"2020-09-01T00:00:00Z","relpermalink":"/dataset/casia-iris-complex/","section":"dataset","summary":"CASIA Iris Image Database In the Complex Scenarios Version 1.0","tags":["Dataset"],"title":"CASIA-Iris-Complex","type":"dataset"},{"authors":["Min Ren"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/# academic/docs/writing-markdown-latex/). -- ","date":1597909378,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597909378,"objectID":"7dc9fb0cd97bbb03983cbff577741d36","permalink":"https://hycasia.github.io/publication/afdr-iris-recognition/afdr-iris-recognition/","publishdate":"2020-08-20T07:42:58Z","relpermalink":"/publication/afdr-iris-recognition/afdr-iris-recognition/","section":"publication","summary":"Alignment Free, Iris Recognition, Preprocessing.","tags":["Source Themes"],"title":"Alignment Free and Distortion Robust Iris Recognition","type":"publication"},{"authors":null,"categories":null,"content":"MediaPipe Iris 实时虹膜跟踪和深度估计 标题居中 -- \u0026emsp;\u0026emsp;包括计算摄影（例如，人像模式和闪光反射）和增强现实效果（例如，虚拟化身）在内的大量实际应用程序都依赖于通过跟踪虹膜来估计眼睛位置。一旦获得了准确的虹膜跟踪，我们就可以确定从相机到用户的距离，而无需使用专用的深度传感器。反过来，这可以改善各种用例，从计算摄影到适当大小的眼镜和帽子的虚拟试戴，到根据视听者的距离采用字体大小的可用性增强。 由于有限的计算资源，可变的光照条件和遮挡物（例如头发或人斜视）的存在，虹膜跟踪是在移动设备上解决的一项艰巨任务。通常，会使用复杂的专用硬件，从而限制了可在其中应用该解决方案的设备范围。   由MediaPipe Iris实现的眼睛重新着色示例  \u0026emsp;\u0026emsp;谷歌日前发布了用于精确虹膜估计的全新机器学习模型：MediaPipe Iris。所述模型以MediaPipe Face Mesh的研究作为基础，而它无需专用硬件就能够通过单个RGB摄像头实时追踪涉及虹膜，瞳孔和眼睛轮廓的界标。利用虹膜界标，模型同时能够在不使用深度传感器的情况下以相对误差小于10％的精度确定对象和摄像头之间的度量距离。请注意，虹膜追踪不会推断人们正在注视的位置，同时不能提供任何形式的身份识别。MediaPipe是一个旨在帮助研究人员和开发者构建世界级机器学习解决方案与应用程序的开源跨平台框架，所以在MediaPipe中实现的这一系统能够支持大多数现代智能手机，PC，笔记本电脑，甚至是Web。   用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定 1. 用于虹膜追踪的机器学习管道\n谷歌介绍道，开发系统的第一步利用了之前针对3D Face Meshes的研究，亦即通过高保真面部界标来生成近似面部几何形状的网格。根据所述网格，研究人员分离出原始图像中的眼睛区域以用于虹膜追踪模型。然后，谷歌将问题分为两个部分：眼睛轮廓估计和虹膜位置。他们设计了一个由一元化编码器组成的多任务模型，每个组件对应一个任务，这样就能够使用特定于任务的训练数据。\n用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定  \u0026emsp;\u0026emsp;为了将裁剪后的眼睛区域用于模型训练，团队手动注释了大约50万张图像。其中，图像涵盖了不同地理位置的各种照明条件和头部姿势，如下所示 用于远视用户的可用性原型：无论距离如何、字体大小能够保持恒定 裁剪的眼睛区域形成模型的输入，而它将通过单独的组件预测界标 2. 虹膜深度：用单个图像进行深度估计 无需任何专门的硬件，这个虹膜追踪模型能够以不到10％的误差确定对象到摄像头的度量距离。相关的原理事实是，人眼的水平直径虹膜基本恒定为11.7±0.5毫米。作为说明，请想象将针孔摄像头模型投影到正方形像素的传感器。你可以使用摄像头的焦距估计从面部界标到对象的距离，而这可以通过Camera Capture API或直接从捕获图像的EXIF元数据，以及其他摄像头固有参数进行获取。给定焦距，对象到摄像头的距离与对象眼睛的物理尺寸成正比，如下图所示\n利用类似的三角形，我们可以根据焦距（f）和虹膜大小来计算对象的距离（d） 左边：在Pixel 2运行的MediaPipe Iris正在以cm为单位估计度量距离，没有采用任何深度摄像头；右边：groud-truth深度 \u0026emsp;\u0026emsp;为了量化所述方法的精确性，研究人员收集了200多位被试的正向同步视频和深度图像，并将其与iPhone 11的深度传感器进行比较。团队使用激光测距设备，通过实验确定iPhone 11的深度传感器在2米以内的误差小于2％。对于使用虹膜大小进行深度估算的方法，平均相对误差为4.3％，标准偏差是2.4％。谷歌对有眼镜被试和正常视力被试（不计入隐形眼镜情况）测试了所述方法，并发现眼镜会将平均相对误差略微提高到4.8％（标准偏差是3.1％）。另外，实验没有测试存在任何眼睛疾病的被试。考虑到MediaPipe Iris不需要专门的硬件，所述结果表明系统能够支持一系列成本范围的设备根据单张图像获取度量深度 估计误差的直方图（左边），以及实际和估计距离的比较（右边） 3. 发布MediaPipe Iris\n\u0026emsp;\u0026emsp;这个虹膜和深度估计模型将作为支持PC，移动设备和Web的跨平台MediaPipe管道发布。正如谷歌在最近一篇关于MediaPipe的博文所述，团队利用WebAssembly和XNNPACK在浏览器中本地运行Iris ML管道，无需将任何数据发送到云端。 使用MediaPipe的WASM堆栈。你可以在浏览器种运行模型 仅使用包含EXIF数据的单张图片计算虹膜深度 4. 未来方向\n\u0026emsp;\u0026emsp;谷歌计划进一步扩展MediaPipe Iris模型，实现更稳定的追踪性能以降低误差，并将其部署用于无障碍用例。谷歌在相关文档和随附的Model Card中详细说明了预期的用途，限制和模型的公平性，从而确保模型的使用符合谷歌的AI原则。请注意，任何形式的监视监控都明显超出应用范围，故不予支持。团队表示：“我们希望的是，通过向广泛的研究与开发社区提供这种虹膜感知功能，从而促使创造性用例的出现，激发负责任的新应用和新研究途径。”","date":1596844800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596844800,"objectID":"81cf5b1dee52c3a2704cf032b84764b9","permalink":"https://hycasia.github.io/post/mediapipe-iris/","publishdate":"2020-08-08T00:00:00Z","relpermalink":"/post/mediapipe-iris/","section":"post","summary":"Real time iris tracking and depth estimation","tags":["Iris Recognition"],"title":"MediaPipe Iris","type":"post"},{"authors":null,"categories":null,"content":" 8月5日晚芒果台，中科院自动化研究所智能感知与计算研究中心助理研究员王云龙老师带你探索虹膜识别的奥秘\n ","date":1596585600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596585600,"objectID":"18b6e328f4025e374d7cf62a484c9871","permalink":"https://hycasia.github.io/post/how-does-iris-recognize-identity-successfully/","publishdate":"2020-08-05T00:00:00Z","relpermalink":"/post/how-does-iris-recognize-identity-successfully/","section":"post","summary":"If the eye is the window of the soul, the iris is equivalent to the ID card of the window. So, what is the secret in our eyes?","tags":["Internal"],"title":"How does iris recognize identity successfully?","type":"post"},{"authors":null,"categories":null,"content":" 7月13日播出的CCTV-1《生活圈》节目中，谭铁牛院士现身为观众介绍了虹膜识别技术。谭铁牛院士介绍到：“虹膜识别是一种相对比较新颖的生物特征识别技术，下一步虹膜识别技术会进一步朝着移动化、便捷化以及和其他的相关的生物特征识别技术，比如人脸识别技术，相融合的方向发展，具有非常广阔的发展空间。”\n  \u0026emsp;\u0026emsp;自动化所孙哲南研究员也携团队相关成果做客节目，具体讲解了虹膜识别技术的优势与应用。  虹膜识别技术的优势  \u0026emsp;\u0026emsp;虹膜识别是利用人眼表面黑色瞳孔和白色巩膜之间圆环状的区域进行身份识别的技术。虹膜识别的优势在于：  第一，虹膜先天具有非常高的唯一性。虹膜中可以发现证明至少244个独立变量来决定其唯一性，而指纹和人脸大概只有十几个或者几十个这样的变量。\n第二，虹膜终身不变。年龄的增长、化妆或者整容可以改变人的容貌，却无法改变虹膜\n 虹膜识别的应用  1. 虹膜识别应用于手机\n孙哲南研究员在节目中展示了团队研发的虹膜识别解锁手机，在手机终端装载虹膜识别模块，直接刷眼就可以解锁手机。防护镜、墨镜甚至黑暗的环境都不会成为虹膜识别的阻碍。\n2. 虹膜识别应用于电脑\n使用虹膜解锁电脑，刷眼后一瞬间即可安全登陆，省去了总是忘记密码与密码被盗的烦恼。\n3. 虹膜识别防盗门锁\n只需对准虹膜采集框，即可解锁开门。团队展示的虹膜锁采用近红外主动光源成像，即使在光线很暗的楼道内，虹膜锁也可以正常工作。\n4. 虹膜识别收费闸机\n想象一下，当我们驾车通过收费闸机时，只需要刷一下眼睛，就可以自动收费抬杆，这是一种什么样的感觉呢？将来，这一系统也可以应用于高速公路ETC中，驾驶员就可以直接通过眼神识别进行缴费。\n其实，虹膜识别的应用远不止这些，并且在不远的将来，它还可以在更多地方得以运用，为人们的生活提供超乎想象的便利！\n","date":1594598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594598400,"objectID":"f23406c5ec08c2fbf3e3e66608b5ebc2","permalink":"https://hycasia.github.io/post/academician-introduces-you-to-iris-recognition/","publishdate":"2020-07-13T00:00:00Z","relpermalink":"/post/academician-introduces-you-to-iris-recognition/","section":"post","summary":"If the eye is the window of the soul, the iris is equivalent to the ID card of the window. So, what is the secret in our eyes?","tags":["Internal"],"title":"Academician introduces you to iris recognition","type":"post"},{"authors":null,"categories":null,"content":"Download the whole database (1.86GB) OR\nDownload the separated subsets below\n          Download CASIA-Iris-Interval (30.9MB)  Download CASIA-Iris-Lamp (390MB)    Download CASIA-Iris-Twins (60MB)  Download CASIA-Iris-Distance(767MB)    Download CASIA-Iris-Thousand (490MB)  Download CASIA-Iris-Syn (171MB)    1. Introduction With the pronounced need for reliable personal identification, iris recognition has become an important enabling technology in our society. Although an iris pattern is naturally an ideal identifier, the development of a high-performance iris recognition algorithm and transferring it from research lab to practical applications is still a challenging task. Automatic iris recognition has to face unpredictable variations of iris images in real-world applications. For example, recognition of iris images of poor quality, nonlinearly deformed iris images, iris images at a distance, iris images on the move, and faked iris images all are open problems in iris recognition. A basic work to solve the problems is to design and develop a high quality iris image database including all these variations. Moreover, a novel iris image database may help identify some frontier problems in iris recognition and leads to a new generation of iris recognition technology.\nCASIA Iris Image Database (CASIA-Iris) developed by our research group has been released to the international biometrics community and updated from CASIA-IrisV1 to CASIA-IrisV3 since 2002. More than 3,000 users from 70 countries or regions have downloaded CASIA-Iris and much excellent work on iris recognition has been done based on these iris image databases. Although great progress of iris recognition has been achieved since 1990s, the rapid growth of iris recognition applications has clearly highlighted two challenges, i.e. usability and scalability.\nUsability is the largest bottleneck of current iris recognition. It is a trend to develop long-range iris image acquisition systems for friendly user authentication. However, iris images captured at a distance are more challenging than traditional close-up iris images. Lack of long-range iris image data in the public domain has hindered the research and development of next-generation iris recognition systems.\nMost current iris recognition methods have been typically evaluated on medium sized iris image databases with a few hundreds of subjects. However, more and more large-scale iris recognition systems are deployed in real-world applications. Many new problems are met in classification and indexing of large-scale iris image databases. So scalability is another challenging issue in iris recognition.\nIn order to promote research on long-range and large-scale iris recognition systems, we are pleased to release to the public domain CASIA Iris Image Database V4.0 (or CASIA-IrisV4 for short).\n2. Brief Descriptions and Statistics of the Database CASIA-IrisV4 is an extension of CASIA-IrisV3 and contains six subsets. The three subsets from CASIA-IrisV3 are CASIA-Iris-Interval, CASIA-Iris-Lamp, and CASIA-Iris-Twins respectively. The three new subsets are CASIA-Iris-Distance, CASIA-Iris-Thousand, and CASIA-Iris-Syn.\nCASIA-IrisV4 contains a total of 54,601 iris images from more than 1,800 genuine subjects and 1,000 virtual subjects. All iris images are 8 bit gray-level JPEG files, collected under near infrared illumination or synthesized. Some statistics and features of each subset are given in Table 1. The six data sets were collected or synthesized at different times and CASIA-Iris-Interval, CASIA-Iris-Lamp, CASIA-Iris-Distance, CASIA-Iris-Thousand may have a small inter-subset overlap in subjects.\n2.1 CASIA-Iris-Interval Iris images of CASIA-Iris-Interval were captured with our self-developed close-up iris camera (Fig.1). The most compelling feature of our iris camera is that we have designed a circular NIR LED array, with suitable luminous flux for iris imaging. Because of this novel design, our iris camera can capture very clear iris images (see Fig.2). CASIA-Iris-Interval is well-suited for studying the detailed texture features of iris images.\nFig.1 The self-developed iris camera used for collection of CASIA-Iris-Interval Fig.2 Example iris images in CASIA-Iris-Interval\n2.2 CASIA-Iris-Lamp CASIA-Iris-Lamp was collected using a hand-held iris sensor produced by OKI (Fig.3). A lamp was turned on/off close to the subject to introduce more intra-class variations when we collected CASIA-Iris-Lamp. Elastic deformation of iris texture (Fig.4) due to pupil expansion and contraction under different illumination conditions is one of the most common and challenging issues in iris recognition. So CASIA-Iris-Lamp is good for studying problems of non-linear iris normalization and robust iris feature representation.\nFig.3 The hand-held iris camera used for collection of CASIA-Iris-Lamp Fig.4 Example iris images in CASIA-Iris-Lamp\n2.3 CASIA-Iris-Twins CASIA-Iris-Twins contains iris images of 100 pairs of twins, which were collected during Annual Twins Festival in Beijing using OKI\u0026rsquo;s IRISPASS-h camera (Fig.5). Although iris is usually regarded as a kind of phenotypic biometric characteristics and even twins have their unique iris patterns, it is interesting to study the dissimilarity and similarity between iris images of twins.\nFig.5 Example iris images in CASIA-Iris-Twins\n2.4 CASIA-Iris-Distance CASIA-Iris-Distance contains iris images captured using our self-developed long-range multi-modal biometric image acquisition and recognition system (LMBS, Fig.6). The advanced biometric sensor can recognize users from 3 meters away by actively searching iris, face or palmprint patterns in the visual field via an intelligent multi-camera imaging system. The LMBS is human-oriented by fusing computer vision, human computer interaction and multi-camera coordination technologies and improves greatly the usability of current biometric systems. The iris images of CASIA-Iris-Distance were captured by a high resolution camera so both dual-eye iris and face patterns are included in the image region of interest (Fig. 7). And detailed facial features such as skin pattern are also visible for multi-modal biometric information fusion.\nFig.6 The biometric sensor used for collection of CASIA-Iris-Distance Fig.7 An example image in CASIA-Iris-Distance\n2.5 CASIA-Iris-Thousand CASIA-Iris-Thousand contains 20,000 iris images from 1,000 subjects, which were collected using IKEMB-100 camera (Fig. 8) produced by IrisKing. IKEMB-100 is a dual-eye iris camera with friendly visual feedback, realizing the effect of “What You See Is What You Get”. The bounding boxes shown in the frontal LCD help users adjust their pose for high-quality iris image acquisition. The main sources of intra-class variations in CASIA-Iris-Thousand are eyeglasses and specular reflections. Since CASIA-Iris-Thousand is the first publicly available iris dataset with one thousand subjects, it is well-suited for studying the uniqueness of iris features and develop novel iris classification and indexing methods.\nFig.8 The iris camera used for collection of CASIA-Iris-Thousand Fig.9 An example image in CASIA-Iris-Thousand\n2.6 CASIA-Iris-Syn CASIA-Iris-Syn contains 10,000 synthesized iris images of 1,000 classes. The iris textures of these images are synthesized automatically from a subset of CASIA-IrisV1 with the approach described in [1] (Fig. 10). Then the iris ring regions were embedded into the real iris images, which makes the artificial iris images more realistic. The intra-class variations introduced into the synthesized iris dataset include deformation, blurring, and rotation, which raise a challenge problem for iris feature representation and matching. We have demonstrated in [1] that the synthesized iris images are visually realistic and most subjects can not distinguish genuine and artificial iris images. More importantly, the performance results tested on the synthesized iris image database have similar statistical characteristics to genuine iris database. So users of CASIA-IrisV4 are encouraged to use CASIA-Iris-Syn for iris recognition research and any suggestions are welcome. If CASIA-Iris-Syn proves to be successful for most researchers of iris recognition, we will provide more and more synthesized iris images in the future.\nFig. 10 Flowchart of the iris texture synthesis method for generation of CASIA-Iris-Syn Fig. 11 Example iris images in CASIA-Iris-Syn\n3. Database Organization The file name of each image in CASIA-IrisV4 is unique to each other and denotes some useful properties associated with the image such as subset category, left/right/double, subject ID, class ID, image ID etc. The file naming rules of all six subsets are listed as follows:\n  The images of CASIA-Iris-Interval are stored as:\nroot_path/CASIA-Iris-Interval/YYY/S1YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\n  The images of CASIA-Iris-Lamp are stored as:\nroot_path/CASIA-Iris-Lamp/YYY/E/S2YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\n  The images of CASIA-Iris-Twins are stored as:\nroot_path/CASIA-Iris-Twins\\XX\\YE\\S3XXYENN.jpg\nXX: the index of family\nY: the identifier to one of the twins\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\n  The images of CASIA-Iris-Distance are stored as:\nroot_path/CASIA-Iris-Distance/YYY/S4YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘D’ denotes dual-eye iris image\nNN: the index of the image in the class\n  The images of CASIA-Iris-Thousand are stored as:\n$ root path$ /CASIA-Iris-Thousand/YYY/E/S5YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘L’ denotes left eye and ‘R’ denotes right eye\nNN: the index of the image in the class\n  The images of CASIA-Iris-Syn are stored as:\nroot_path/CASIA-Iris-Syn/YYY/S6YYYENN.jpg\nYYY: the unique identifier of the subject in the subset\nE: ‘S’ denotes it is a synthesized iris image\nNN: the index of the image in the class\n  4. Copyright Note and Contacts The database is released for research and educational purposes. We hold no liability for any undesirable consequences of using the database. All rights of the CASIA database are reserved. Any person or organization is not permitted to distribute, publish, copy, or disseminate this database. In all documents and papers that report experimental results based on this database, our efforts in constructing the database should be acknowledged such as “Portions of the research in this paper use the CASIA-IrisV4 collected by the Chinese Academy of Sciences\u0026rsquo; Institute of Automation (CASIA)” and a reference to “CASIA Iris Image Database, http://biometrics.idealtest.org/” should be included. A copy of all reports and papers that are for public or general release that use the CASIA-IrisV4 should be forwarded upon release or publication to:\nProfessor Tieniu Tan\nCenter for Biometrics and Security Research\nNational Laboratory of Pattern Recognition\nInstitute of Automation, Chinese Academy of Sciences\nP.O.Box 2728\nBeijing 100190\nChina\nor send electronic copies to znsun@nlpr.ia.ac.cn.\nQuestions regarding this database can be addressed to Dr. Zhenan Sun at\nDr. Zhenan Sun\nCenter for Biometrics and Security Research\nNational Laboratory of Pattern Recognition\nInstitute of Automation, Chinese Academy of Sciences\nP.O.Box 2728\nBeijing 100190\nChina\nTel: +86 10 8261 0278\nFax: +86 10 6255 1993\nEmail: znsun@nlpr.ia.ac.cn\nPublications  Tieniu Tan, Zhaofeng He, Zhenan Sun, \u0026ldquo;Efficient and robust segmentation of noisy iris images for non-cooperative iris recognition\u0026rdquo;, Image and Vision Computing, Vol.28, No. 2, 2010, pp.223-230. T. Tan and L. Ma, “Iris Recognition: Recent Progress and Remaining Challenges”, Proc. of SPIE, Vol. 5404, pp. 183-194, 12-13 Apr 2004, Orlando, USA. Zhenan Sun, Tieniu Tan, \u0026ldquo;Ordinal Measures for Iris Recognition,\u0026rdquo; IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 12, 2009, pp. 2211 - 2226. Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, \u0026ldquo;Towards Accurate and Fast Iris Segmentation for Iris Biometrics\u0026rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 31, No. 9, 2009, pp.1670 - 1684. L. Ma, T. Tan, Y. Wang and D. Zhang, “Personal Identification Based on Iris Texture Analysis”, IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), Vol. 25, No. 12, pp.1519-1533, 2003. Li Ma, Tieniu Tan, Yunhong Wang and Dexin Zhang, “Efficient Iris Recognition by Characterizing Key Local Variations”, IEEE Trans. on Image Processing, Vol. 13, No.6, pp. 739- 750, 2004. L. Ma, T. Tan, D. Zhang and Y. Wang, “Local Intensity Variation Analysis for Iris Recognition, Pattern Recognition”, Vol.37, No.6, pp. 1287-1298, 2004. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, IEEE Transactions on Systems, Man, and Cybernetics-Part Cï¼ŒVolume 35, Issue 3, 2005, pp.435 - 441. Zhenan Sun, Tieniu Tan, Yunhong Wang, “Robust Encoding of Local Ordinal Measures: A General Framework of Iris Recognition”, Proceedings of International Workshop on Biometric Authentication (BioAW), Lecture Notes in Computer Science, Vol.3087, 2004, pp. 270-282. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Improving Iris Recognition Accuracy via Cascaded Classifiers”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 418-425. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Robust Direction Estimation of Gradient Vector Field for Iris Recognition”, Proceedings of the 17th International Conference on Pattern Recognition, Vol.2, 2004, pp.783-786. Zhenan Sun, Yunhong Wang, Tieniu Tan, Jiali Cui, “Cascading Statistical And Structural Classifiers For Iris Recognition”, Proceedings of IEEE International Conference on Image Processing, 2004, pp.1261-1264. Zhenan Sun, Tieniu Tan, Yunhong Wang, “Iris Recognition Based on Non-local Comparisons”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 67-77. Zhenan Sun, Tieniu Tan, and Xianchao Qiu, \u0026ldquo;Graph Matching Iris Image Blocks with Local Binary Pattern\u0026rdquo;, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 366-372. Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418. Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, “Robust and Fast Assessment of Iris Image Quality”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 464 - 471. Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “An Appearance-Based Method for Iris Detection”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp.1091-1096, 2004, Korea. Jiali Cui, Yunhong Wang, Junzhou Huang, Tieniu Tan, Zhenan Sun and Li Ma, “An Iris Image Synthesis Method Based on PCA and Super-Resolution”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 4, pp. 471-474, 23-26 August 2004, Cambridge, UK. Jiali Cui, Li Ma, Yunhong Wang, Tieniu Tan and Zhenan Sun, “A Fast and Robust Iris Localization Method Based on Texture Segmentation”, Proc. of SPIE, Vol. 5404, pp. 401-408, 2004, USA. Jiali Cui, Yunhong Wang, Li Ma, Tieniu Tan and Zhenan Sun, “An Iris Recognition Algorithm Using Local Extreme Points”, Proceedings of the 1st International Conference on Biometric Authentication, Lecture Notes in Computer Science, Vol.3072, 2004, pp. 442-449. Jiali Cui, Yunhong Wang, Tieniu Tan and Zhenan Sun, “Fast Recursive Mathematical Morphological Transforms”, Proc. of the 3rd International Conference on Image and Graphics (ICIG), pp. 422-425, 2004, Hong Kong. Junzhou Huang, Tieniu Tan, Li Ma, and Yunhong Wang, Phase Correlation Based Iris Image Registration Model, Journal of Computer Science and Technology, Vol.20, No.3, pp.419-425, May 2005. L. Ma, Y. Wang and T. Tan, “Iris Recognition Based on Multichannel Gabor Filtering”, Proc. of the 5th Asian Conference on Computer Vision (ACCV), Vol. I, pp.279-283, Jan 22-25, 2002, Melbourne, Australia. L. Ma, Y. Wang and T. Tan, “Iris Recognition Using Circular Symmetric Filters”, Proc. of IAPR International Conference on Pattern Recognitionï¼ˆICPRï¼‰, Vol. II, pp. 414-417, August 11-15, 2002, Quebec, Canada. J. Z. Huang, L. Ma, T. N. Tan and Y. H. Wang, “Learning-Based Enhancement Model of Iris”, Proc. of British Machine Vision Conference (BMVC), pp. 153-162, 2003. J. Z. Huang, L. Ma, and Y. H. Wang and T. N. Tan, “Iris Model Based on Local Orientation Description”, Proc. of the 6th Asian Conference on Computer Vision (ACCV), Vol.2, pp. 954-959, 2004, Korea. J. Z. Huang, Y. H. Wang, T. N. Tan and J. L. Cui, “A New Iris Segmentation Model”, Proc. of the 17th IAPR International Conference on Pattern Recognition (ICPR), Vol. 3, pp. 554-557, 23-26 August 2004, Cambridge, UK. J. Z. Huang, Y. H. Wang, J. L. Cui and T. N. Tan, “Noise Removal and Impainting Model for Iris Image”, Proc. of IEEE International Conference on Image Processing (ICIP), pp. 869-872, 2004, Singapore. Yuqing He, Yangsheng Wang and Tieniu Tan, “Iris Image Capture System Design For Personal Identification”, Proceedings of the 5th Chinese Conference on Biometric Recognition, Lecture Notes in Computer Science, Vol.3338, 2004, pp. 546-552. Zhuoshi Wei, Tieniu Tan, Zhenan Sun, Jiali Cui, \u0026ldquo;Robust and Fast Assessment of Iris Image quality\u0026rdquo;, Proc. of International Conference of Biometrics, pp. 464-471, 2006. Zhuoshi Wei, Tieniu Tan and Zhenan Sun, \u0026ldquo;Nonlinear Iris Deformation Correction Based on Gaussian Model\u0026rdquo;, International Conference of Biometrics, pp 780-789, 2007. Zhuoshi Wei, Yufei Han, Zhenan Sun and Tieniu Tan, Palmprint Image Synthesis: A Preliminary Study, Proc. of IEEE International Conference on Image Processing, 2008. Zhuoshi Wei, Tieniu Tan and Zhenan Sun, Synthesis of Large Realistic Iris Databases Using Patch-based Sampling, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008. Zhuoshi Wei, Xianchao Qiu, Zhenan Sun and Tieniu Tan, Counterfeit Iris Detection Based on Texture Analysis, Proc. of IEEE International Conference on Pattern Recognition (ICPR), 2008. Zhaofeng He, Tieniu Tan and Zhenan Sun, “Iris Localization via Pulling and Pushing”, Proc. of the 18th IEEE International Conference on Pattern Recognition (ICPR'06), Vol.4, pp. 366-369, 2006, Hongkong. Zhaofeng He, Tieniu Tan, Zhenan Sun, Xianchao Qiu, Cheng Zhong and Wenbo Dong, Boosting Ordinal Features for Iris Recognition, Proc. of the 26th IEEE International Conference on Computer Vision and Pattern Recognition (CVPR’08) , pp. 1-8, June 23-28, Alaska, USA Zhaofeng He, Zhenan Sun, Tieniu Tan and Xianchao Qiu, Enhanced Usability of Iris Recognition via Efficient User Interface and Iris Image Restoration, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California Accepted. Zhaofeng He, Tieniu Tan, Zhenan Sun and Xianchao Qiu, Robust Eyelid, Eyelash and Shadow Localization for Iris Recognition”, Proc. of the 15th IEEE International Conference on Image Processing (ICIP’08), 2008, San Diego, California, Accepted. Zhaofeng He, Tieniu Tan, Zhenan Sun and Zhuoshi Wei, “Efficient Iris Spoof Detection via Boosted Local Binary Patterns”, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1080-1090, 2009. Xianchao Qiu, Zhenan Sun, Tieniu Tan, “Global Texture Analysis of Iris Images for Ethnic Classification”, Proceedings of International Conference on Biometrics, Lecture Notes in Computer Sciences, Vol. 3832, 2005, pp. 411 - 418. Xianchao Qiu, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Coarse Iris Classification by Learned Visual Dictionary\u0026rdquo;, In Proc. of The 2nd International Conference on Biometrics, pp. 770–779, Seoul, Korea, Aug. 2007. Xianchao Qiu, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Global Texture Analysis of Iris Images for Ethnic Classification\u0026rdquo;, In Proc. of The 1st International Conference on Biometrics, pp. 411–418, Hong Kong, China. Jan. 2006. Wenbo Dong, Zhenan Sun, Tieniu Tan, Xianchao Qiu, Self-adaptive iris image acquisition system, Proc. SPIE vol. 6944, 1-9, 2008. Wenbo Dong, Zhenan Sun, Tieniu Tan, How to make iris recognition easier?, Proc. of the 19th International Conference on Pattern Recognition, pp.1-4, 2008. Wenbo Dong, Zhenan Sun, Tieniu Tan, Zhuoshi Wei, \u0026ldquo;Quality-based dynamic threshold for iris matching\u0026rdquo;, In Proceedings of IEEE International Conference on Image Processing, 2009. Long Zhang, Zhenan Sun, Tieniu Tan and Shungeng Hu, \u0026ldquo;Robust Biometric Key Extraction Based on Iris Cryptosystem\u0026rdquo;, Proc. of the Third International Conference on Biometrics, Lecture Notes in Computer Science, Vol.5558, pp.1060-1069, 2009. Hui Zhang, Zhenan Sun, and Tieniu Tan, Contact lens detection based on weighted LBP, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010. Hui Zhang, Zhenan Sun, and Tieniu Tan, Statistics of Local Surface Curvatures for Mis-Localized Iris Detection, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010. Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Texture Removal for Adaptive Level Set based Iris Segmentation\u0026rdquo;, The 17th IEEE International Conference on Image Processing (ICIP2010), Hong Kong, China, 2010. Xiaobo Zhang, Zhenan Sun, and Tieniu Tan, \u0026ldquo;Hierarchical Fusion of Face and Iris for Personal Identification\u0026rdquo;, The 20th IEEE International Conference on Pattern Recognition (ICPR2010), Istanbul, Turkey, 2010.  ","date":1594598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594598400,"objectID":"ae1e247fa12af13a5e4adbf9c5bd5314","permalink":"https://hycasia.github.io/dataset/casia-irisv4/","publishdate":"2020-07-13T00:00:00Z","relpermalink":"/dataset/casia-irisv4/","section":"dataset","summary":"CASIA Iris Image Database Version 4.0","tags":["Internal"],"title":"CASIA-IrisV4","type":"dataset"},{"authors":["Yunlong Wang","Kunbo Zhang","Zhenan Sun"],"categories":[],"content":"","date":1593698267,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593698267,"objectID":"1b1eef3547be9058f1fe6e4e353e3184","permalink":"https://hycasia.github.io/publication/wang-icpr-2020/","publishdate":"2020-07-02T21:57:47+08:00","relpermalink":"/publication/wang-icpr-2020/","section":"publication","summary":"The primitive basis of image based material recognition builds upon the fact that discrepancies in the reflectances of distinct materials lead to imaging differences under multiple viewpoints. LF cameras possess coherent abilities to capture multiple sub-aperture views (SAIs) within one exposure, which can provide appropriate multi-view sources for material recognition. In this paper, a unified Factorize-Connect-Merge (FCM) deep-learning pipeline is proposed to solve problems of light field image based material recognition. 4D light-field data as input is initially decomposed into consecutive 3D light-field slices. Shallow CNN is leveraged to extract low-level visual features of each view inside these slices. As to establish correspondences between these SAIs, Bidirectional Long-Short Term Memory (Bi-LSTM) network is built upon these low-level features to model the imaging differences. After feature selection including concatenation and dimension reduction, effective and robust feature representations for material recognition can be extracted from 4D light-field data. Experimental results indicate that the proposed pipeline can obtain remarkable performances on both tasks of single-pixel material classification and whole-image material segmentation. In addition, the proposed pipeline can potentially benefit and inspire other researchers who may also take LF images as input and need to extract 4D light-field representations for computer vision tasks such as object classification, semantic segmentation and edge detection. ","tags":["Light Field Imaging","Material Recognition"],"title":"A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition","type":"publication"},{"authors":["Yunlong Wang","Fei Liu","Kunbo Zhang","Zilei Wang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1593697016,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593697016,"objectID":"2109ff2c79baba13b70eeb80dca5a2c1","permalink":"https://hycasia.github.io/publication/wang-tci-2020/","publishdate":"2020-07-02T21:36:56+08:00","relpermalink":"/publication/wang-tci-2020/","section":"publication","summary":"Multi-view properties of light field (LF) imaging enable exciting applications such as auto-refocusing, depth estimation and 3D reconstruction. However, limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards more practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. We have proposed an end-to-end deep learning framework named Pseudo 4DCNN to solve these problems in a conference paper. Rethinking on the overall paradigm, we further extend pseudo 4DCNN and propose a novel loss function which is applicable for all tasks of light field reconstruction i.e. EPI Structure Preserving (ESP) loss function. This loss function is proposed to attenuate the blurry edges and artifacts caused by averaging effect of L2 norm based loss function. Furthermore, the extended Pseudo 4DCNN is compared with recent state-of-the-art (SOTA) approaches on more publicly available light field databases, as well as self-captured light field biometrics and microscopy datasets. Experimental results demonstrate that the proposed framework can achieve better performances than vanilla Pseudo 4DCNN and other SOTA methods, especially in the terms of visual quality under occlusions. The source codes and self-collected datasets for reproducibility are available online.","tags":["Light Field Imaging","Light Field Reconstruction","View Synthesis"],"title":"High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN","type":"publication"},{"authors":["Ping Song","Ling Huang","Yunlong Wang","Fei Liu","Zhenan Sun"],"categories":[],"content":"","date":1581931385,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581931385,"objectID":"b0deb47fed2de9246b6fc2ad2759db60","permalink":"https://hycasia.github.io/publication/song-automatica2019/","publishdate":"2020-02-17T17:23:05+08:00","relpermalink":"/publication/song-automatica2019/","section":"publication","summary":"Light-field (LF) imaging is a new method to capture both intensity and direction information of visual objects, providing promising solutions to biometrics. Iris recognition is a reliable personal identification method, however it is also vulnerable to spoofing attacks, such as iris patterns printed on contact lens or paper. Therefore iris liveness detection is an important module in iris recognition systems. In this paper, an iris liveness detection approach is proposed to take full advantages of intrinsic characteristics in light-field iris imaging. LF iris images are captured by using lab-made LF cameras, based on which the geometric features as well as the texture features are extracted using the LF digital refocusing technology. These features are combined for genuine and fake iris image classification. Experiments were carried out based on the self-collected near-infrared LF iris database, and the average classification error rate (ACER) of the proposed method is 3.69%, which is 5.94% lower than the best state-of-the-art method. Experimental results indicate the proposed method is able to work effectively and accurately to prevent spoofing attacks such as printed and screen-displayed iris input attacks.","tags":["Smart Iris Recognition","Light Field Imaging","Liveliness Detection"],"title":"Iris Liveness Detection Based on Light Field Imaging","type":"publication"},{"authors":["Kunbo Zhang","Zhenteng Shen","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1581927125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581927125,"objectID":"31a28568fc82801d805f164fe4fa795c","permalink":"https://hycasia.github.io/publication/zhang-ijcb2020/","publishdate":"2020-09-01T08:58:18Z","relpermalink":"/publication/zhang-ijcb2020/","section":"publication","summary":"Imaging volume of an iris recognition system has been restricting the throughput and cooperation convenience in biometric applications. Numerous improvement trials are still impractical to supersede the dominant fixed-focus lens in stand-off iris recognition due to incremental performance increase and complicated optical design. In this study, we develop a novel all-in-focus iris imaging system using a focus-tunable lens and a 2D steering mirror to greatly extend capture volume by spatiotemporal multiplexing method. Our iris imaging depth of field extension system requires no mechanical motion and is capable to adjust the focal plane at extremely high speed. In addition, the motorized reflection mirror adaptively steers the light beam to extend the horizontal and vertical field of views in an active manner. The proposed all-in-focus iris camera increases the depth of field up to 3.9 m which is a factor of37.5 compared with conventional long focal lens. We also experimentally demonstrate the capability of this 3D light beam steering imaging system in real-time multi-person iris refocusing using dynamic focal stacks and the potential of continuous iris recognition for moving participants.","tags":["smart iris recognition"],"title":"All-in-Focus Iris Camera With a Great Capture Volume","type":"publication"},{"authors":["Leyuan Wang","Kunbo Zhang","Min Ren","Yunlong Wang","Zhenan Sun"],"categories":[],"content":"","date":1581927125,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581927125,"objectID":"de635ff7cbab6751ee97da39bf0486bc","permalink":"https://hycasia.github.io/publication/wang-ijcb2020/","publishdate":"2020-09-01T08:58:18Z","relpermalink":"/publication/wang-ijcb2020/","section":"publication","summary":"A large portion of iris images captured in real world scenarios are poor quality due to the uncontrolled environment and the non-cooperative subject. To ensure that the recognition algorithm is not affected by low-quality images, traditional hand-crafted factors based methods discard most images, which will cause system timeout and disrupt user experience. In this paper, we propose a recognition-oriented quality metric and assessment method for iris image to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factors based iris quality assessment methods. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. Compared with hand-crafted factors based methods, the proposed method is a trial to bridge the gap between the image quality assessment and biometric recognition. ","tags":["smart iris recognition","Image Quality Assessment (IQA)"],"title":"Recognition Oriented Iris Image Quality Assessment in the Feature Space","type":"publication"},{"authors":["Fei Liu","Shubo Zhou","Yunlong Wang","Guangqi Hou","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3c85d24c9d3f90d582905c2dad27d460","permalink":"https://hycasia.github.io/publication/liu-tip-2020/","publishdate":"2020-02-16T11:44:49.291517Z","relpermalink":"/publication/liu-tip-2020/","section":"publication","summary":"Binocular stereo vision (SV) has been widely used to reconstruct the depth information, but it is quite vulnerable to scenes with strong occlusions. As an emerging computational photography technology, light-field (LF) imaging brings about a novel solution to passive depth perception by recording multiple angular views in a single exposure. In this paper, we explore binocular SV and LF imaging to form the binocular-LF imaging system. An imaging theory is derived by modeling the imaging process and analyzing disparity properties based on the geometrical optics theory. Then an accurate occlusion-robust depth estimation algorithm is proposed by exploiting multibaseline stereo matching cues and defocus cues. The occlusions caused by binocular SV and LF imaging are detected and handled to eliminate the matching ambiguities and outliers. Finally, we develop a binocular-LF database and capture realworld scenes by our binocular-LF system to test the accuracy and robustness. The experimental results demonstrate that the proposed algorithm definitely recovers high quality depth maps with smooth surfaces and precise geometric shapes, which tackles the drawbacks of binocular SV and LF imaging simultaneously.","tags":["Light Field Imaging","Binocular Light-Field","Imaging Theory","Occlusion-Robust Depth Perception Application","Depth Estimation"],"title":"Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application","type":"publication"},{"authors":["Min Ren","Yunlong Wang","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"42391abf460f06da4099f460e39f9575","permalink":"https://hycasia.github.io/publication/ren-2020-dynamic/","publishdate":"2020-02-16T11:44:49.304474Z","relpermalink":"/publication/ren-2020-dynamic/","section":"publication","summary":"The generalization ability of Convolutional neural networks (CNNs) for biometrics drops greatly due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrated the merits of both CNNs and graphical models to learn dynamic graph representations for occlusion problems in biometrics, called Dynamic Graph Representation (DGR). Convolutional features onto certain regions are re-crafted by a graph generator to establish the connections among the spatial parts of biometrics and build Feature Graphs based on these node representations. Each node of Feature Graphs corresponds to a specific part of the input image and the edges express the spatial relationships between parts. By analyzing the similarities between the nodes, the framework is able to adaptively remove the nodes representing the occluded parts. During dynamic graph matching, we propose a novel strategy to measure the distances of both nodes and adjacent matrixes. In this way, the proposed method is more convincing than CNNs-based methods because the dynamic graph method implies a more illustrative and reasonable inference of the biometrics decision. Experiments conducted on iris and face demonstrate the superiority of the proposed framework, which boosts the accuracy of occluded biometrics recognition by a large margin comparing with baseline methods.","tags":null,"title":"Dynamic Graph Representation for Occlusion Handling in Biometrics","type":"publication"},{"authors":["Caiyong Wang","Yunlong Wang","Yunfan Liu","Zhaofeng He","Ran He","Zhenan Sun"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f4f5b8312490d53a2b9be623c9456692","permalink":"https://hycasia.github.io/publication/wang-tbiom-2020/","publishdate":"2020-02-16T11:44:49.293511Z","relpermalink":"/publication/wang-tbiom-2020/","section":"publication","summary":"Accurate sclera segmentation is critical for successful sclera recognition. However, studies on sclera segmentation algorithms are still limited in the literature. In this paper, we propose a novel sclera segmentation method based on the improved U-Net model, named as ScleraSegNet. We perform in-depth analysis regarding the structure of U-Net model, and propose to embed an attention module into the central bottleneck part between the contracting path and the expansive path of U-Net to strengthen the ability of learning discriminative representations. We compare different attention modules and find that channel-wise attention is the most effective in improving the performance of the segmentation network. Besides, we evaluate the effectiveness of data augmentation process in improving the generalization ability of the segmentation network. Experiment results show that the best performing configuration of the proposed method achieves state-of-the-art performance with F-measure values of 91.43%, 89.54% on UBIRIS.v2 and MICHE, respectively.","tags":["Smart Iris Recognition","Sclera segmentation","sclera recognition","U-net","attention mechanism","SSBC"],"title":"ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation","type":"publication"},{"authors":["Junxing Hu","Hui Zhang","Lihu Xiao","Jing Liu","Xingguang Li","Zhaofeng He"],"categories":["Biometrics","Computer Vision"],"content":"","date":1557273600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557273600,"objectID":"3427756f1cbf3d871c8ee7196e18c40a","permalink":"https://hycasia.github.io/publication/hu-2019-icb/","publishdate":"2020-02-16T11:44:49.3025Z","relpermalink":"/publication/hu-2019-icb/","section":"publication","summary":"In this paper, we present an end-to-end model, namely Seg-Edge bilateral constraint network. The iris edge map generated from rich convolutional layers optimize the iris segmentation by aligning it with the iris boundary. The iris region produced by the coarse segmentation limits the scope. It makes the edge filtering pay more attention to the interesting target. We compress the model while keeping the performance levels almost intact and even better by using l1-norm. The proposed model advances the state-of-the-art iris segmentation accuracies.","tags":["Iris Segmentation","Bilateral Constrained Domain Transform","Model Pruning"],"title":"Seg-Edge Bilateral Constraint Network for Iris Segmentation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask\n Documentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://hycasia.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Min Ren","Caiyong Wang","Yunlong Wang","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7ed96a89a31d1e4cf5ae2812d6a8560b","permalink":"https://hycasia.github.io/publication/ren-2019-alignment/","publishdate":"2020-02-16T11:44:49.3025Z","relpermalink":"/publication/ren-2019-alignment/","section":"publication","summary":"Iris recognition is a reliable personal identification method but there is still much room to improve its accuracy especially in less-constrained situations. For example, free movement of head pose may cause large rotation difference between iris images. And illumination variations may cause irregular distortion of iris texture. To match intra-class iris images with head rotation robustly, the existing soadminlutions usually need a precise alignment operation by exhaustive search within a determined range in iris image preprosessing or brute-force searching the minimum Hamming distance in iris feature matching. In the wild enviroments, iris rotation is of much greater uncertainty than that in constrained situations and exhaustive search within a determined range is impracticable. This paper presents a unified feature-level solution to both alignment free and distortion robust iris recognition in the wild. A new deep learning based method named Alignment Free Iris Network (AFINet) is  proposed, which utilizes a trainable VLAD (Vector of Locally Aggregated Descriptors) encoder called NetVLAD [18] to decouple the correlations between local representations and their spatial positions. And deformable convolution [5] is leveraged to overcome iris texture distortion by dense adaptive sampling. The results of extensive experiments on three public iris image databases and the simulated degradation databases show that AFINet significantly outperforms state-of-art iris recognition methods.","tags":null,"title":"Alignment Free and Distortion Robust Iris Recognition","type":"publication"},{"authors":["Jianze Wei","Yunlong Wang","Xiang Wu","Zhaofeng  He","Ran He","Zhenan Sun"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"4ef9c0699a3577c43bd7614ef612239c","permalink":"https://hycasia.github.io/publication/csin-btas-2019/","publishdate":"2020-02-16T11:44:49.305472Z","relpermalink":"/publication/csin-btas-2019/","section":"publication","summary":"Due to the growing demand of iris biometrics, lots of new sensors are being developed for high-quality image acquisition. However, upgrading the sensor and re-enrolling for users is expensive and time-consuming. This leads to a dilemma where enrolling on one type of sensor but recognizing on the others. For this cross-sensor matching, the large gap between distributions of enrolling and recognizing images usually results in degradation in recognition performance. To alleviate this degradation, we propose Cross-sensor iris network (CSIN) by applying the adversarial strategy and weakening interference of sensor-specific information. Specifically, there are three valuable efforts towards learning discriminative iris features. Firstly, the proposed CSIN adds extra feature extractors to generate residual components containing sensor-specific information and then utilizes these components to narrow the distribution gap. Secondly, an adversarial strategy is borrowed from Generative Adversarial Networks to align feature distributions and further reduce the discrepancy of images caused by sensors. Finally, we extend triplet loss and propose instance-anchor loss to pull the instances of the same class together and push away from others. It is worth mentioning that the proposed method doesn’t need pair-same data or triplet, which reduced the cost of data preparation. Experiments on two real-world datasets validate the effectiveness of the proposed method in cross-sensor iris recognition.","tags":null,"title":"Cross-sensor iris recognition using adversarial strategy and sensor-specific information","type":"publication"},{"authors":["Yunlong Wang","Fei Liu","Kunbo Zhang","Guangqi Hou","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"eb4d0743b49b915838e1c049f782c32d","permalink":"https://hycasia.github.io/publication/wang-tip-2018/","publishdate":"2020-02-16T11:44:49.308464Z","relpermalink":"/publication/wang-tip-2018/","section":"publication","summary":"The low spatial resolution of light-field image poses significant difficulties in exploiting its advantage. To mitigate the dependency of accurate depth or disparity information as priors for light-field image super-resolution, we propose an implicitly multi-scale fusion scheme to accumulate contextual information from multiple scales for super-resolution reconstruction. The implicitly multi-scale fusion scheme is then incorporated into bidirectional recurrent convolutional neural network, which aims to iteratively model spatial relations between horizontally or vertically adjacent sub-aperture images of light-field data. Within the network, the recurrent convolutions are modified to be more effective and flexible in modeling the spatial correlations between neighboring views. A horizontal sub-network and a vertical sub-network of the same network structure are ensembled for final outputs via stacked generalization. Experimental results on synthetic and real-world data sets demonstrate that the proposed method outperforms other state-of-the-art methods by a large margin in peak signal-to-noise ratio and gray-scale structural similarity indexes, which also achieves superior quality for human visual systems. Furthermore, the proposed method can enhance the performance of light field applications such as depth estimation.","tags":["Light Field Imaging","Deep Learning","Light-Field Image Super Resolution","LFNet","Bidirectional Recurrent Convolutional Neural Network"],"title":"LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution","type":"publication"},{"authors":["Zihui Yan","Lingxiao He","Man Zhang","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1516406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1516406400,"objectID":"e0620a1e63047a625c9029c0a432199b","permalink":"https://hycasia.github.io/publication/yan-2018-liveness/","publishdate":"2018-05-20T00:00:00Z","relpermalink":"/publication/yan-2018-liveness/","section":"publication","summary":"In modern society, iris recognition has become increasingly popular. The security risk of iris recognition is increasing rapidly because of the attack by various patterns of fake iris. A German hacker organization called Chaos Computer Club cracked the iris recognition system of Samsung Galaxy S8 recently. In view of these risks, iris liveness detection has shown its significant importance to iris recognition systems. The state-of-the-art algorithms mainly rely on hand-crafted texture features which can only identify fake iris images with single pattern. In this paper, we proposed a Hierarchical Multiclass Iris Classification (HMC) for liveness detection based on CNN. HMC mainly focuses on iris liveness detection of multipattern fake iris. The proposed method learns the features of different fake iris patterns by CNN and classifies the genuine or fake iris images by hierarchical multi-class classification. This classification takes various characteristics of different fake iris patterns into account. All kinds of fake iris patterns are divided into two categories by their fake areas. The process is designed as two steps to identify two categories of fake iris images respectively. Experimental results demonstrate an extremely higher accuracy of iris liveness detection than other state-of-the-art algorithms. The proposed HMC remarkably achieves the best results with nearly 100% accuracy on NDContact, CASIA-Iris-Interval, CASIA-Iris-Syn and LivDetIris-2017-Warsaw datasets. The method also achieves the best results with 100% accuracy on a hybrid dataset which consists of ND-Contact and LivDet-Iris-2017-Warsaw dataset","tags":["Iris-liveness-detection"],"title":"Hierarchical Multi-class Iris Classification for Liveness Detection","type":"publication"},{"authors":["Yunlong Wang","Fei Liu","Zilei Wang","Guangqi Hou","Zhenan Sun","Tieniu Tan"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"7ea1d1915c9430ad95008a9a2d7ba623","permalink":"https://hycasia.github.io/publication/wang-eccv-2018/","publishdate":"2020-02-16T11:44:49.306438Z","relpermalink":"/publication/wang-eccv-2018/","section":"publication","summary":"Limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. In this paper, an end-to-end deep learning framework is proposed to solve these problems by exploring Pseudo 4DCNN. Specifically, 2D strided convolutions operated on stacked EPIs and detail-restoration 3D CNNs connected with angular conversion are assembled to build the Pseudo 4DCNN. The key advantage is to efficiently synthesize dense 4D light fields from a sparse set of input views. The learning framework is well formulated as an entirely trainable problem, and all the weights can be recursively updated with standard backpropagation. The proposed framework is compared with state-of-the-art approaches on both genuine and synthetic light field databases, which achieves significant improvements of both image quality (+2 dB higher) and computational efficiency (over 10X faster). Furthermore, the proposed framework shows good performances in real-world applications such as biometrics and depth estimation.","tags":["Computational Photography","Deep Learning","Light Field Reconstruction","End-to-end View Synthesis","Pseudo 4DCNN"],"title":"End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN","type":"publication"},{"authors":["Min Ren","Lingxiao He","Haiqing Li","Yunfan Liu","Zhenan Sun","Tieniu Tan"],"categories":[],"content":"","date":1500508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1500508800,"objectID":"d536e7e8dd5e6fb4e646edaaaa38a4dd","permalink":"https://hycasia.github.io/publication/ren-ccbr-2017/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/publication/ren-ccbr-2017/","section":"publication","summary":"In this paper, we study the problem of partial person reidentification (re-id). This problem is more difficult than general person re-identification because the body in probe image is not full. We propose a novel method, similarity-guided sparse representation (SG-SR), as a robust solution to improve the discrimination of the sparse coding. There are three main components in our method. In order to include multi-scale information, a dictionary consisting of features extracted from multiscale patches is established in the first stage. A low rank constraint is then enforced on the dictionary based on the observation that its subspaces of each class should have low dimensions. After that, a classification model is built based on a novel similarity-guided sparse representation which can choose vectors that are more similar to the probe feature vector. The results show that our method outperforms existing partial person re-identification methods significantly and achieves state-of-theart accuracy.","tags":[""],"title":"Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation","type":"publication"},{"authors":null,"categories":null,"content":" \u0026emsp;\u0026emsp;2017年4月27日，微软获得了一项虹膜识别技术的专利，未来该技术将纳入 Windows Hello ，用于微软旗下的智能手机、笔记本等设备中。   \u0026emsp;\u0026emsp;虹膜识别是生物识别技术中的一种。其他的生物识别方法包括人脸、指纹、声音、视网膜、静脉识别等，而由于人类虹膜上拥有266个特征点，远高于其他生物识别技术的不到60个特征点，故被认为具有更高的精准性和安全性。  虹膜识别是通过数学算法对人眼虹膜特征进行编码和对比的身份识别方法。根据专利文件描述，微软的智能设备可以从两个或者三个方向照明中拍摄用户眼睛的多张照片。每个角度的眼睛照片都能检测虹膜特征并创建不同的数据点。\n \u0026emsp;\u0026emsp;微软在其专利申请中指出，人眼是部分透明的三维结构。当光通过瞳孔传递到眼睛的视网膜上。从不同的方向用光照射眼睛，就可以获得许多图像帧的图像数据，并对至少两个图像帧的数据进行对比，找到相似的地方，获得相关的数据，这些数据与关注的眼睛区域是一致的。然后系统根据数据自动确定假眼睛的验证特点，从而用来验证真正的眼睛。 ","date":1492473600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1492473600,"objectID":"980c4d848fd82695ddc84c8f35ba3d09","permalink":"https://hycasia.github.io/post/iris-recognition-is-the-general-trend/","publishdate":"2017-04-18T00:00:00Z","relpermalink":"/post/iris-recognition-is-the-general-trend/","section":"post","summary":"Microsoft has applied for relevant patents.","tags":["Iris Recognition"],"title":"Iris recognition is the general trend?","type":"post"},{"authors":["Yunlong Wang","Guangqi Hou","Zhenan Sun","Zilei Wang","Tieniu Tan"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"2928bf922f6dfea0b2816c5dda87461e","permalink":"https://hycasia.github.io/publication/wang-icip-2016/","publishdate":"2020-02-16T11:44:49.30946Z","relpermalink":"/publication/wang-icip-2016/","section":"publication","summary":"","tags":["Light Field Image Processing","Super Resolution"],"title":"A simple and robust super resolution method for light field images","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2ad8a763bd2d0252e637ae3d73d00357","permalink":"https://hycasia.github.io/benchmark/iris-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/iris-recognition/","section":"benchmark","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Iris Recognition"],"title":"Iris Recognition","type":"benchmark"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"800539ac7a5d691dc12638e217d01344","permalink":"https://hycasia.github.io/benchmark/light-field-photography/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/light-field-photography/","section":"benchmark","summary":"An example of using the in-built project page.","tags":["Internal"],"title":"Light Field Photography","type":"benchmark"},{"authors":null,"categories":null,"content":"Light field The light field is a vector function that describes the amount of light flowing in every direction through every point in space. The space of all possible light rays is given by the five-dimensional plenoptic function, and the magnitude of each ray is given by the radiance. Michael Faraday was the first to propose (in an 1846 lecture entitled \u0026ldquo;Thoughts on Ray Vibrations\u0026rdquo;) that light should be interpreted as a field, much like the magnetic fields on which he had been working for several years. The phrase light field was coined by Andrey Gershun in a classic paper on the radiometric properties of light in three-dimensional space (1936).\nThe 4D light field In a plenoptic function, if the region of interest contains a concave object (think of a cupped hand), then light leaving one point on the object may travel only a short distance before being blocked by another point on the object. No practical device could measure the function in such a region.\nHowever, if we restrict ourselves to locations outside the convex hull (think shrink-wrap) of the object, i.e. in free space, then we can measure the plenoptic function by taking many photos using a digital camera. Moreover, in this case the function contains redundant information, because the radiance along a ray remains constant from point to point along its length, as shown at left. In fact, the redundant information is exactly one dimension, leaving us with a four-dimensional function (that is, a function of points in a particular four-dimensional manifold). Parry Moon dubbed this function the photic field (1981), while researchers in computer graphics call it the 4D light field (Levoy 1996) or Lumigraph (Gortler 1996). Formally, the 4D light field is defined as radiance along rays in empty space.\nThe set of rays in a light field can be parameterized in a variety of ways, a few of which are shown below. Of these, the most common is the two-plane parameterization shown at right (below). While this parameterization cannot represent all rays, for example rays parallel to the two planes if the planes are parallel to each other, it has the advantage of relating closely to the analytic geometry of perspective imaging. Indeed, a simple way to think about a two-plane light field is as a collection of perspective images of the st plane (and any objects that may lie astride or beyond it), each taken from an observer position on the uv plane. A light field parameterized this way is sometimes called a light slab.\nWays to create light fields Light fields are a fundamental representation for light. As such, there are as many ways of creating light fields as there are computer programs capable of creating images or instruments capable of capturing them.\nIn computer graphics, light fields are typically produced either by rendering a 3D model or by photographing a real scene. In either case, to produce a light field views must be obtained for a large collection of viewpoints. Depending on the parameterization employed, this collection will typically span some portion of a line, circle, plane, sphere, or other shape, although unstructured collections of viewpoints are also possible (Buehler 2001).\nDevices for capturing light fields photographically may include a moving handheld camera or a robotically controlled camera (Levoy 2002), an arc of cameras (as in the bullet time effect used in The Matrix), a dense array of cameras (Kanade 1998; Yang 2002; Wilburn 2005), handheld cameras (Ng 2005; Georgiev 2006; Marwah 2013), microscopes (Levoy 2006), or other optical system (Bolles 1987).\nHow many images should be in a light field? The largest known light field (of Michelangelo\u0026rsquo;s statue of Night) contains 24,000 1.3-megapixel images. At a deeper level, the answer depends on the application. For light field rendering (see the Application section below), if you want to walk completely around an opaque object, then of course you need to photograph its back side. Less obviously, if you want to walk close to the object, and the object lies astride the st plane, then you need images taken at finely spaced positions on the uv plane (in the two-plane parameterization shown above), which is now behind you, and these images need to have high spatial resolution.\nThe number and arrangement of images in a light field, and the resolution of each image, are together called the \u0026ldquo;sampling\u0026rdquo; of the 4D light field. Analyses of light field sampling have been undertaken by many researchers; a good starting point is Chai (2000). Also of interest is Durand (2005) for the effects of occlusion, Ramamoorthi (2006) for the effects of lighting and reflection, and Ng (2005) and Zwicker (2006) for applications to plenoptic cameras and 3D displays, respectively.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"f3939b65ca5c42e2fb3f6da617eea401","permalink":"https://hycasia.github.io/project/light-field-photography/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/light-field-photography/","section":"project","summary":"Light field is a parameterized representation of four-dimensional radiation field which contains both position and direction information in space","tags":["Light Field Photography"],"title":"Light Field Photography","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ca512dce072f572757e9d9864616a179","permalink":"https://hycasia.github.io/benchmark/periocular-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/periocular-recognition/","section":"benchmark","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Ocular Recognition"],"title":"Periocular Recognition","type":"benchmark"},{"authors":null,"categories":null,"content":"To be updated.....","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"ac06c4cd588437d58836c7231f598f37","permalink":"https://hycasia.github.io/project/periocular-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/periocular-recognition/","section":"project","summary":"Face recognition derivative, periocular recognition","tags":["Ocular Recognition"],"title":"Periocular Recognition","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"51f301e55709d9e7c1f85ba49df9fc22","permalink":"https://hycasia.github.io/benchmark/sclera-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/sclera-recognition/","section":"benchmark","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Ocular Recognition"],"title":"Sclera Recognition","type":"benchmark"},{"authors":null,"categories":null,"content":"To be updated.....","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"b6c4b4b975d4de2d1e4d92dadf9663ae","permalink":"https://hycasia.github.io/project/sclera-recognition/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/sclera-recognition/","section":"project","summary":"Another ocular biometric after iris biometrics","tags":["Ocular Recognition"],"title":"Sclera Recognition","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"3abe601028448bda026fbb372863f404","permalink":"https://hycasia.github.io/benchmark/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/benchmark/internal-project/","section":"benchmark","summary":"An example of using the in-built project page.","tags":["Demo"],"title":"The Other Internal Project","type":"benchmark"},{"authors":["Guangqi Hou","Chi Zhang","Yunlong Wang","Zhenan Sun"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"001f7d5877030a5fcb34529429dfd9cb","permalink":"https://hycasia.github.io/publication/hou-spie-2016/","publishdate":"2020-02-16T11:44:49.311463Z","relpermalink":"/publication/hou-spie-2016/","section":"publication","summary":"Counting the number of people is still an important task in social security applications, and a few methods based on video surveillance have been proposed in recent years. In this paper, we design a novel optical sensing system to directly acquire the depth map of the scene from one light-field camera. The light-field sensing system can count the number of people crossing the passageway, and record the direction and intensity of rays at a snapshot without any assistant light devices. Depth maps are extracted from the raw light-ray sensing data. Our smart sensing system is equipped with a passive imaging sensor, which is able to naturally discern the depth difference between the head and shoulders for each person. Then a human model is built. Through detecting the human model from light-field images, the number of people passing the scene can be counted rapidly. We verify the feasibility of the sensing system as well as the accuracy by capturing real-world scenes passing single and multiple people under natural illumination.","tags":["Light field photography","RGB-D sensor","People Counting"],"title":"4D light-field sensing system for people counting","type":"publication"}]