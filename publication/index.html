<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="The smart iris research (SIR) is affiliated to the Institute of Automation, Chinese Academy of Sciences (CAS).">

  
  <link rel="alternate" hreflang="en-us" href="https://hycasia.github.io/publication/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
    
      
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
    
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  
  <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="SIR-Smart Iris Recognition">
  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://hycasia.github.io/publication/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="SIR-Smart Iris Recognition">
  <meta property="og:url" content="https://hycasia.github.io/publication/">
  <meta property="og:title" content="Publications | SIR-Smart Iris Recognition">
  <meta property="og:description" content="The smart iris research (SIR) is affiliated to the Institute of Automation, Chinese Academy of Sciences (CAS)."><meta property="og:image" content="https://hycasia.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://hycasia.github.io/images/icon_hu42e11727b1bd363727a38d52796c9ff5_15822_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    <meta property="og:updated_time" content="2020-07-02T21:57:47&#43;08:00">
  

  




  


  





  <title>Publications | SIR-Smart Iris Recognition</title>

</head>
<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  









<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">SIR-Smart Iris Recognition</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">SIR-Smart Iris Recognition</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#head"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#benchmark"><span>Benchmark</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/dataset"><span>Datasets</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>Member</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  












  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            
            <option value=".pubtype-1">
              Conference paper
            </option>
            
            <option value=".pubtype-2">
              Journal article
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2020">
              2020
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2018">
              2018
            </option>
            
            <option value=".year-2017">
              2017
            </option>
            
            <option value=".year-2016">
              2016
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>25th International Conference on Pattern Recognition (ICPR2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-icpr-2020/">
      <img src="/publication/wang-icpr-2020/featured_hub6bc0afe46e9e9022c0e1013b8bc7e51_322063_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-icpr-2020/">A Novel Deep-learning Pipeline for Light Field Image Based Material Recognition</a>
  </h3>

  
  <a href="/publication/wang-icpr-2020/" class="summary-link">
    <div class="article-style">
      <p>The primitive basis of image based material recognition builds upon the fact that discrepancies in the reflectances of distinct materials lead to imaging differences under multiple viewpoints. LF cameras possess coherent abilities to capture multiple sub-aperture views (SAIs) within one exposure, which can provide appropriate multi-view sources for material recognition. In this paper, a unified Factorize-Connect-Merge (FCM) deep-learning pipeline is proposed to solve problems of light field image based material recognition. 4D light-field data as input is initially decomposed into consecutive 3D light-field slices. Shallow CNN is leveraged to extract low-level visual features of each view inside these slices. As to establish correspondences between these SAIs, Bidirectional Long-Short Term Memory (Bi-LSTM) network is built upon these low-level features to model the imaging differences. After feature selection including concatenation and dimension reduction, effective and robust feature representations for material recognition can be extracted from 4D light-field data. Experimental results indicate that the proposed pipeline can obtain remarkable performances on both tasks of single-pixel material classification and whole-image material segmentation. In addition, the proposed pipeline can potentially benefit and inspire other researchers who may also take LF images as input and need to extract 4D light-field representations for computer vision tasks such as object classification, semantic segmentation and edge detection.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-icpr-2020/wang-icpr-2020.pdf" target="_blank" rel="noopener">
  PDF
</a>







  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Fei Liu</span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span >Zilei Wang</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Computational Imaging</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tci-2020/">
      <img src="/publication/wang-tci-2020/featured_hu34f7cca5134d38775f830752b4ff7dee_172015_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tci-2020/">High-fidelity View Synthesis for Light Field Imaging with Extended Pseudo 4DCNN</a>
  </h3>

  
  <a href="/publication/wang-tci-2020/" class="summary-link">
    <div class="article-style">
      <p>Multi-view properties of light field (LF) imaging enable exciting applications such as auto-refocusing, depth estimation and 3D reconstruction. However, limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards more practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. We have proposed an end-to-end deep learning framework named Pseudo 4DCNN to solve these problems in a conference paper. Rethinking on the overall paradigm, we further extend pseudo 4DCNN and propose a novel loss function which is applicable for all tasks of light field reconstruction i.e. EPI Structure Preserving (ESP) loss function. This loss function is proposed to attenuate the blurry edges and artifacts caused by averaging effect of L2 norm based loss function. Furthermore, the extended Pseudo 4DCNN is compared with recent state-of-the-art (SOTA) approaches on more publicly available light field databases, as well as self-captured light field biometrics and microscopy datasets. Experimental results demonstrate that the proposed framework can achieve better performances than vanilla Pseudo 4DCNN and other SOTA methods, especially in the terms of visual quality under occlusions. The source codes and self-collected datasets for reproducibility are available online.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/9061053" target="_blank" rel="noopener">
  PDF
</a>




  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/wylcasia/ExtendedP4DCNN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TCI.2020.2986092" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span >Ping Song</span>, <span >Ling Huang</span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Fei Liu</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Acta Automatica Sinica
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/song-automatica2019/">
      <img src="/publication/song-automatica2019/featured_hua6cf0c9f82465dfbb2af54a0f5684fb0_1059934_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="Iris Liveness Detection Based on Light Field Imaging">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/song-automatica2019/">Iris Liveness Detection Based on Light Field Imaging</a>
  </h3>

  
  <a href="/publication/song-automatica2019/" class="summary-link">
    <div class="article-style">
      <p>Light-field (LF) imaging is a new method to capture both intensity and direction information of visual objects, providing promising solutions to biometrics. Iris recognition is a reliable personal identification method, however it is also vulnerable to spoofing attacks, such as iris patterns printed on contact lens or paper. Therefore iris liveness detection is an important module in iris recognition systems. In this paper, an iris liveness detection approach is proposed to take full advantages of intrinsic characteristics in light-field iris imaging. LF iris images are captured by using lab-made LF cameras, based on which the geometric features as well as the texture features are extracted using the LF digital refocusing technology. These features are combined for genuine and fake iris image classification. Experiments were carried out based on the self-collected near-infrared LF iris database, and the average classification error rate (ACER) of the proposed method is 3.69%, which is 5.94% lower than the best state-of-the-art method. Experimental results indicate the proposed method is able to work effectively and accurately to prevent spoofing attacks such as printed and screen-displayed iris input attacks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/song-automatica2019/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.16383/j.aas.c180213" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/leyuan-wang/">Leyuan Wang</a></span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    February 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Joint Conference on Biometrics 2020 (IJCB2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-ijcb2020/">
      <img src="/publication/wang-ijcb2020/featured_huf62c02a8356c55152bbb2cab3f6b47df_86627_918x517_fill_q90_lanczos_smart1_2.png" class="article-banner" alt="Recognition Oriented Iris Image Quality Assessment in the Feature Space">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-ijcb2020/">Recognition Oriented Iris Image Quality Assessment in the Feature Space</a>
  </h3>

  
  <a href="/publication/wang-ijcb2020/" class="summary-link">
    <div class="article-style">
      <p>A large portion of iris images captured in real world applications are poor quality due to limited depth of field and dynamic user pose. It is difficult to achieve a good balance between iris image quality criteria and high throughput of personal identification. This paper proposes a recognition-oriented method for iris image quality assessment to deal with the problem. The method regards the iris image embeddings Distance in Feature Space (DFS) as the quality metric and the prediction is based on deep neural networks with the attention mechanism. The relationship between Image Rejection Rate (IRR) and Equal Error Rate (EER) is proposed to evaluate the performance of the quality assessment algorithm under the same image quality distribution and the same recognition algorithm. The quality metric proposed in this paper can significantly improve the performance of the recognition algorithm while reducing the number of images discarded for recognition, which is advantageous over hand-crafted factor based iris quality assessment methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  





<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-ijcb2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Yunfan Liu</span>, <span >Zhaofeng He</span>, <span >Ran He</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tbiom-2020/">
      <img src="/publication/wang-tbiom-2020/featured_huc6676f3f0515d4a98830b7b111a2a6bf_1212330_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tbiom-2020/">ScleraSegNet: An Attention Assisted U-Net Model for Accurate Sclera Segmentation</a>
  </h3>

  
  <a href="/publication/wang-tbiom-2020/" class="summary-link">
    <div class="article-style">
      <p>Accurate sclera segmentation is critical for successful sclera recognition. However, studies on sclera segmentation algorithms are still limited in the literature. In this paper, we propose a novel sclera segmentation method based on the improved U-Net model, named as ScleraSegNet. We perform in-depth analysis regarding the structure of U-Net model, and propose to embed an attention module into the central bottleneck part between the contracting path and the expansive path of U-Net to strengthen the ability of learning discriminative representations. We compare different attention modules and find that channel-wise attention is the most effective in improving the performance of the segmentation network. Besides, we evaluate the effectiveness of data augmentation process in improving the generalization ability of the segmentation network. Experiment results show that the best performing configuration of the proposed method achieves state-of-the-art performance with F-measure values of 91.43%, 89.54% on UBIRIS.v2 and MICHE, respectively.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8987270" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tbiom-2020/cite.bib">
  Cite
</button>





  
    
  

  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/sclera-recognition/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TBIOM.2019.2962190" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI2020)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-2020-dynamic/">
      <img src="/publication/ren-2020-dynamic/featured_hu5a7c5f350ab04416a3b81176db425b79_598277_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="Dynamic Graph Representation for Occlusion Handling in Biometrics">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-2020-dynamic/">Dynamic Graph Representation for Occlusion Handling in Biometrics</a>
  </h3>

  
  <a href="/publication/ren-2020-dynamic/" class="summary-link">
    <div class="article-style">
      <p>The generalization ability of Convolutional neural networks (CNNs) for biometrics drops greatly due to the adverse effects of various occlusions. To this end, we propose a novel unified framework integrated the merits of both CNNs and graphical models to learn dynamic graph representations for occlusion problems in biometrics, called Dynamic Graph Representation (DGR). Convolutional features onto certain regions are re-crafted by a graph generator to establish the connections among the spatial parts of biometrics and build Feature Graphs based on these node representations. Each node of Feature Graphs corresponds to a specific part of the input image and the edges express the spatial relationships between parts. By analyzing the similarities between the nodes, the framework is able to adaptively remove the nodes representing the occluded parts. During dynamic graph matching, we propose a novel strategy to measure the distances of both nodes and adjacent matrixes. In this way, the proposed method is more convincing than CNNs-based methods because the dynamic graph method implies a more illustrative and reasonable inference of the biometrics decision. Experiments conducted on iris and face demonstrate the superiority of the proposed framework, which boosts the accuracy of occluded biometrics recognition by a large margin comparing with baseline methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/ren-2020-dynamic/ren-2020-dynamic.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-2020-dynamic/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2020">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span >Fei Liu</span>, <span >Shubo Zhou</span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Guangqi Hou</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2020
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Image Processing</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/liu-tip-2020/">
      <img src="/publication/liu-tip-2020/featured_hu38c5c190bc09914eb7aa3bcdf8d34db0_176628_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/liu-tip-2020/">Binocular Light-Field: Imaging Theory and Occlusion-Robust Depth Perception Application</a>
  </h3>

  
  <a href="/publication/liu-tip-2020/" class="summary-link">
    <div class="article-style">
      <p>Binocular stereo vision (SV) has been widely used to reconstruct the depth information, but it is quite vulnerable to scenes with strong occlusions. As an emerging computational photography technology, light-field (LF) imaging brings about a novel solution to passive depth perception by recording multiple angular views in a single exposure. In this paper, we explore binocular SV and LF imaging to form the binocular-LF imaging system. An imaging theory is derived by modeling the imaging process and analyzing disparity properties based on the geometrical optics theory. Then an accurate occlusion-robust depth estimation algorithm is proposed by exploiting multibaseline stereo matching cues and defocus cues. The occlusions caused by binocular SV and LF imaging are detected and handled to eliminate the matching ambiguities and outliers. Finally, we develop a binocular-LF database and capture realworld scenes by our binocular-LF system to test the accuracy and robustness. The experimental results demonstrate that the proposed algorithm definitely recovers high quality depth maps with smooth surfaces and precise geometric shapes, which tackles the drawbacks of binocular SV and LF imaging simultaneously.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://ieeexplore.ieee.org/document/8851410/" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/liu-tip-2020/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIP.2019.2943019" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/junxing-hu/">Junxing Hu</a></span>, <span >Hui Zhang</span>, <span >Lihu Xiao</span>, <span >Jing Liu</span>, <span >Xingguang Li</span>, <span >Zhaofeng He</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    May 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/biometrics/">Biometrics</a>, <a href="/category/computer-vision/">Computer Vision</a></span>
  

</div>

  

  
  
  
  
  <a href="/publication/hu-2019-icb/">
      <img src="/publication/hu-2019-icb/featured_hub20b5cffd4ef0267b382a51e7f712c34_752909_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="Seg-Edge Bilateral Constraint Network for Iris Segmentation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hu-2019-icb/">Seg-Edge Bilateral Constraint Network for Iris Segmentation</a>
  </h3>

  
  <a href="/publication/hu-2019-icb/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we present an end-to-end model, namely Seg-Edge bilateral constraint network. The iris edge map generated from rich convolutional layers optimize the iris segmentation by aligning it with the iris boundary. The iris region produced by the coarse segmentation limits the scope. It makes the edge filtering pay more attention to the interesting target. We compress the model while keeping the performance levels almost intact and even better by using l1-norm. The proposed model advances the state-of-the-art iris segmentation accuracies.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/hu-2019-icb/hu-2019-ICB.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hu-2019-icb/cite.bib">
  Cite
</button>



  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="http://www.cbsr.ia.ac.cn/china/Iris%20Databases%20CH.asp" target="_blank" rel="noopener">
  Dataset
</a>



  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/jianze-wei/">Jianze Wei</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Xiang Wu</span>, <span >Zhaofeng  He</span>, <span >Ran He</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>10th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS2019)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/csin-btas-2019/">
      <img src="/publication/csin-btas-2019/featured_hu921313c5a5a034189cb86b41e165c587_214059_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="Cross-sensor iris recognition using adversarial strategy and sensor-specific information">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/csin-btas-2019/">Cross-sensor iris recognition using adversarial strategy and sensor-specific information</a>
  </h3>

  
  <a href="/publication/csin-btas-2019/" class="summary-link">
    <div class="article-style">
      <p>Due to the growing demand of iris biometrics, lots of new sensors are being developed for high-quality image acquisition. However, upgrading the sensor and re-enrolling for users is expensive and time-consuming. This leads to a dilemma where enrolling on one type of sensor but recognizing on the others. For this cross-sensor matching, the large gap between distributions of enrolling and recognizing images usually results in degradation in recognition performance. To alleviate this degradation, we propose Cross-sensor iris network (CSIN) by applying the adversarial strategy and weakening interference of sensor-specific information. Specifically, there are three valuable efforts towards learning discriminative iris features. Firstly, the proposed CSIN adds extra feature extractors to generate residual components containing sensor-specific information and then utilizes these components to narrow the distribution gap. Secondly, an adversarial strategy is borrowed from Generative Adversarial Networks to align feature distributions and further reduce the discrepancy of images caused by sensors. Finally, we extend triplet loss and propose instance-anchor loss to pull the instances of the same class together and push away from others. It is worth mentioning that the proposed method doesnâ€™t need pair-same data or triplet, which reduced the cost of data preparation. Experiments on two real-world datasets validate the effectiveness of the proposed method in cross-sensor iris recognition.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/csin-btas-2019/csin-btas-2019.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/csin-btas-2019/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span ><a href="/author/caiyong-wang/">Caiyong Wang</a></span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>International Conference on Biometrics (ICB2019)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-2019-alignment/">
      <img src="/publication/ren-2019-alignment/featured_hu007bcafc10ae98032c8ba59182b80711_89939_918x517_fill_q90_lanczos_smart1_2.png" class="article-banner" alt="Alignment Free and Distortion Robust Iris Recognition">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-2019-alignment/">Alignment Free and Distortion Robust Iris Recognition</a>
  </h3>

  
  <a href="/publication/ren-2019-alignment/" class="summary-link">
    <div class="article-style">
      <p>Iris recognition is a reliable personal identification method but there is still much room to improve its accuracy especially in less-constrained situations. For example, free movement of head pose may cause large rotation difference between iris images. And illumination variations may cause irregular distortion of iris texture. To match intra-class iris images with head rotation robustly, the existing soadminlutions usually need a precise alignment operation by exhaustive search within a determined range in iris image preprosessing or brute-force searching the minimum Hamming distance in iris feature matching. In the wild enviroments, iris rotation is of much greater uncertainty than that in constrained situations and exhaustive search within a determined range is impracticable. This paper presents a unified feature-level solution to both alignment free and distortion robust iris recognition in the wild. A new deep learning based method named Alignment Free Iris Network (AFINet) is  proposed, which utilizes a trainable VLAD (Vector of Locally Aggregated Descriptors) encoder called NetVLAD [18] to decouple the correlations between local representations and their spatial positions. And deformable convolution [5] is leveraged to overcome iris texture distortion by dense adaptive sampling. The results of extensive experiments on three public iris image databases and the simulated degradation databases show that AFINet significantly outperforms state-of-art iris recognition methods.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/ren-2019-alignment/ren-2019-alignment.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-2019-alignment/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Fei Liu</span>, <span ><a href="/author/kunbo-zhang/">Kunbo Zhang</a></span>, <span >Guangqi Hou</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>IEEE Transactions on Image Processing (TIP)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-tip-2018/">
      <img src="/publication/wang-tip-2018/featured_hu42a9c883ceb04f955374599571821e76_1258113_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-tip-2018/">LFNet: A Novel Bidirectional Recurrent Convolutional Neural Network for Light-Field Image Super-Resolution</a>
  </h3>

  
  <a href="/publication/wang-tip-2018/" class="summary-link">
    <div class="article-style">
      <p>The low spatial resolution of light-field image poses significant difficulties in exploiting its advantage. To mitigate the dependency of accurate depth or disparity information as priors for light-field image super-resolution, we propose an implicitly multi-scale fusion scheme to accumulate contextual information from multiple scales for super-resolution reconstruction. The implicitly multi-scale fusion scheme is then incorporated into bidirectional recurrent convolutional neural network, which aims to iteratively model spatial relations between horizontally or vertically adjacent sub-aperture images of light-field data. Within the network, the recurrent convolutions are modified to be more effective and flexible in modeling the spatial correlations between neighboring views. A horizontal sub-network and a vertical sub-network of the same network structure are ensembled for final outputs via stacked generalization. Experimental results on synthetic and real-world data sets demonstrate that the proposed method outperforms other state-of-the-art methods by a large margin in peak signal-to-noise ratio and gray-scale structural similarity indexes, which also achieves superior quality for human visual systems. Furthermore, the proposed method can enhance the performance of light field applications such as depth estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-tip-2018/wang-tip-2018.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-tip-2018/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/wylcasia/LFNet_Test" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/TIP.2018.2834819" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/zihui-yan/">Zihui Yan</a></span>, <span >Lingxiao He</span>, <span >Man Zhang</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/yan-2018-liveness/">
      <img src="/publication/yan-2018-liveness/featured_hue4460b8c05352970f1f202eab563cde8_30715_918x517_fill_q90_lanczos_smart1_2.png" class="article-banner" alt="Hierarchical Multi-class Iris Classification for Liveness Detection">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/yan-2018-liveness/">Hierarchical Multi-class Iris Classification for Liveness Detection</a>
  </h3>

  
  <a href="/publication/yan-2018-liveness/" class="summary-link">
    <div class="article-style">
      <p>In modern society, iris recognition has become increasingly popular. The security risk of iris recognition is increasing rapidly because of the attack by various patterns of fake iris. A German hacker organization called Chaos Computer Club cracked the iris recognition system of Samsung Galaxy S8 recently. In view of these risks, iris liveness detection has shown its significant importance to iris recognition systems. The state-of-the-art algorithms mainly rely on hand-crafted texture features which can only identify fake iris images with single pattern. In this paper, we proposed a Hierarchical Multiclass Iris Classification (HMC) for liveness detection based on CNN. HMC mainly focuses on iris liveness detection of multipattern fake iris. The proposed method learns the features of different fake iris patterns by CNN and classifies the genuine or fake iris images by hierarchical multi-class classification. This classification takes various characteristics of different fake iris patterns into account. All kinds of fake iris patterns are divided into two categories by their fake areas. The process is designed as two steps to identify two categories of fake iris images respectively. Experimental results demonstrate an extremely higher accuracy of iris liveness detection than other state-of-the-art algorithms. The proposed HMC remarkably achieves the best results with nearly 100% accuracy on NDContact, CASIA-Iris-Interval, CASIA-Iris-Syn and LivDetIris-2017-Warsaw datasets. The method also achieves the best results with 100% accuracy on a hybrid dataset which consists of ND-Contact and LivDet-Iris-2017-Warsaw dataset</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/yan-2018-liveness/yan-2018-liveness.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/yan-2018-liveness/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/code_link" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/iris-recognition/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Fei Liu</span>, <span >Zilei Wang</span>, <span >Guangqi Hou</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>European Conference On Computer Vision (ECCV2018)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-eccv-2018/">
      <img src="/publication/wang-eccv-2018/featured_hu89b223e321a4ca8216deff9ea011293b_984792_918x517_fill_q90_lanczos_center_2.png" class="article-banner" alt="End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-eccv-2018/">End-to-End View Synthesis for Light Field Imaging with Pseudo 4DCNN</a>
  </h3>

  
  <a href="/publication/wang-eccv-2018/" class="summary-link">
    <div class="article-style">
      <p>Limited angular resolution has become the main bottleneck of microlens-based plenoptic cameras towards practical vision applications. Existing view synthesis methods mainly break the task into two steps, i.e. depth estimating and view warping, which are usually inefficient and produce artifacts over depth ambiguities. In this paper, an end-to-end deep learning framework is proposed to solve these problems by exploring Pseudo 4DCNN. Specifically, 2D strided convolutions operated on stacked EPIs and detail-restoration 3D CNNs connected with angular conversion are assembled to build the Pseudo 4DCNN. The key advantage is to efficiently synthesize dense 4D light fields from a sparse set of input views. The learning framework is well formulated as an entirely trainable problem, and all the weights can be recursively updated with standard backpropagation. The proposed framework is compared with state-of-the-art approaches on both genuine and synthetic light field databases, which achieves significant improvements of both image quality (+2Â dB higher) and computational efficiency (over 10X faster). Furthermore, the proposed framework shows good performances in real-world applications such as biometrics and depth estimation.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-eccv-2018/wang-eccv-2018.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-eccv-2018/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://github.com/wylcasia/Pseudo4DCNN" target="_blank" rel="noopener">
  Code
</a>




  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2017">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/min-ren/">Min Ren</a></span>, <span >Lingxiao He</span>, <span >Haiqing Li</span>, <span >Yunfan Liu</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July 2017
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/ren-ccbr-2017/">
      <img src="/publication/ren-ccbr-2017/featured_hud016b3b807fef695e860c6ba6c857519_259448_918x517_fill_q90_lanczos_smart1_2.PNG" class="article-banner" alt="Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/ren-ccbr-2017/">Robust Partial Person Re-Identification Based on Similarity-Guided Sparse Representation</a>
  </h3>

  
  <a href="/publication/ren-ccbr-2017/" class="summary-link">
    <div class="article-style">
      <p>In this paper, we study the problem of partial person reidentification (re-id). This problem is more difficult than general person re-identification because the body in probe image is not full. We propose a novel method, similarity-guided sparse representation (SG-SR), as a robust solution to improve the discrimination of the sparse coding. There are three main components in our method. In order to include multi-scale information, a dictionary consisting of features extracted from multiscale patches is established in the first stage. A low rank constraint is then enforced on the dictionary based on the observation that its subspaces of each class should have low dimensions. After that, a classification model is built based on a novel similarity-guided sparse representation which can choose vectors that are more similar to the probe feature vector. The results show that our method outperforms existing partial person re-identification methods significantly and achieves state-of-theart accuracy.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/ren-ccbr-2017/ren-ccbr-2017.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/ren-ccbr-2017/cite.bib">
  Cite
</button>


  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/code_link" target="_blank" rel="noopener">
  Code
</a>




  
    
  











  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2016">
          
            





  






<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span >Guangqi Hou</span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>, <span >Zilei Wang</span>, <span >Tieniu Tan</span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>2016 IEEE International Conference on Image Processing (ICIP2016)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/wang-icip-2016/">
      <img src="/publication/wang-icip-2016/featured_hu3c02e33041bfa332ff03a96bef775598_2084884_918x517_fill_q90_lanczos_smart1_2.png" class="article-banner" alt="A simple and robust super resolution method for light field images">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/wang-icip-2016/">A simple and robust super resolution method for light field images</a>
  </h3>

  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/publication/wang-icip-2016/wang-icip-2016.pdf" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/wang-icip-2016/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1109/ICIP.2016.7532600" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2016">
          
            





  





  


<div class="card-simple">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  
  <span >Guangqi Hou</span>, <span >Chi Zhang</span>, <span ><a href="/author/yunlong-wang/">Yunlong Wang</a></span>, <span ><a href="/author/zhenan-sun/">Zhenan Sun</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    January 2016
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <em>Smart Photonic and Optoelectronic Integrated Circuits XVIII (SPIE2016)</em>
    
  </span>
  

  

  
  
  

  
  

</div>

  

  
  
  
  
  <a href="/publication/hou-spie-2016/">
      <img src="/publication/hou-spie-2016/featured_hubf8459d431c9dd9ea3d4bee732584890_509196_918x517_fill_q90_lanczos_smart1_2.png" class="article-banner" alt="4D light-field sensing system for people counting">
  </a>
  

  <h3 class="article-title mb-1 mt-3">
    <a href="/publication/hou-spie-2016/">4D light-field sensing system for people counting</a>
  </h3>

  
  <a href="/publication/hou-spie-2016/" class="summary-link">
    <div class="article-style">
      <p>Counting the number of people is still an important task in social security applications, and a few methods based on video surveillance have been proposed in recent years. In this paper, we design a novel optical sensing system to directly acquire the depth map of the scene from one light-field camera. The light-field sensing system can count the number of people crossing the passageway, and record the direction and intensity of rays at a snapshot without any assistant light devices. Depth maps are extracted from the raw light-ray sensing data. Our smart sensing system is equipped with a passive imaging sensor, which is able to naturally discern the depth difference between the head and shoulders for each person. Then a human model is built. Through detecting the human model from light-field images, the number of people passing the scene can be counted rapidly. We verify the feasibility of the sensing system as well as the accuracy by capturing real-world scenes passing single and multiple people under natural illumination.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  
    
  



<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1117/12.2212974" target="_blank" rel="noopener">
  PDF
</a>



<button type="button" class="btn btn-outline-primary my-1 mr-1 btn-sm js-cite-modal"
        data-filename="/publication/hou-spie-2016/cite.bib">
  Cite
</button>





  
  <a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="/project/light-field-photography/">
    Project
  </a>
  









<a class="btn btn-outline-primary my-1 mr-1 btn-sm" href="https://doi.org/10.1117/12.2212974" target="_blank" rel="noopener">
  DOI
</a>



  </div>
  

</div>

          
        </div>

        
      </div>

    </div>
  </div>
</div>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.8/mermaid.min.js" integrity="sha256-lyWCDMnMeZiXRi7Zl54sZGKYmgQs4izcT7+tKc+KUBk=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.66c553246b0f279a03be6e5597f72b52.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    
    Published with
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic Website Builder</a>
    

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
